---
title: \sffamily{\bfseries{\LARGE Self-Assessment Works! Paper Spanish Analysis}}
geometry: "left=1.0in,right=1.0in,top=.75in,bottom=.75in"
author: "Steven J. Pierce & Xiaowan Zhang"
output:
 pdf_document:
   latex_engine: xelatex
   number_sections: true
   toc: yes
   toc_depth: 3
urlcolor: blue
header-includes:
- \usepackage{fancyhdr}
- \usepackage[yyyymmdd,hhmmss]{datetime}
- \usepackage{lastpage}
- \usepackage{amsmath,amsthm}
- \usepackage{unicode-math}
- \defaultfontfeatures{Ligatures=TeX}
- \usepackage[font={small}, margin=1cm, skip=2pt]{caption}
- \usepackage{url}
- \usepackage{floatrow} 
- \floatplacement{figure}{!ht}
- \floatplacement{table}{!ht}
- \usepackage{placeins}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{dcolumn}
- \usepackage{titling}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage{lscape}
- \pagestyle{fancy}
- \lhead{Self-Assessment Works! (Spanish)}
- \rhead{\today\ \currenttime}
- \cfoot{ }
- \lfoot{\texttt{\footnotesize \detokenize{`r sub(".Rmd", ".pdf", knitr:::current_input(dir = FALSE))`}}} 
- \fancyfoot[R]{\thepage}
- \renewcommand{\headrulewidth}{0.4pt}
- \renewcommand{\footrulewidth}{0.4pt}
- \fancypagestyle{plain}{\pagestyle{fancy}}
---

\FloatBarrier

# PURPOSE
This file facilitates reproducing the results reported in our manuscript (Winke,
Zhang, & Pierce, 2019), which was based on a presentation (Winke, Pierce, &
Zhang 2018). It analyzes data on Spanish language learners who took a Can-Do
self-assessment test, along with the more authoritative OPIc language
proficiency test. We did both correlation analyses and continuation-ratio models
that examine the effect of course and OPIc speaking proficiency scores on the
passing rate for each level of the Can-Do statements self-assessment. The
objective was to validate the Can-Do test results.

\FloatBarrier

## Target Journal
We currently plan to submit this as a "Research Report" to a journal called 
Studies in Second Language Acquisition (SSLA), 
https://www.cambridge.org/core/journals/studies-in-second-language-acquisition.
The author instructions are at
https://www.cambridge.org/core/journals/studies-in-second-language-acquisition/information/instructions-contributors.

We will consider applying for the SSLA *Open Data Badge* and the 
*Open Materials Badge* described at 
https://www.cambridge.org/core/journals/studies-in-second-language-acquisition/open-science-badges
. Note that hosting a public Github repository should meet the requirements for 
both badges because GitHub is on the 
[Registry of Research Data Repositories](https://www.re3data.org). 

\FloatBarrier

## Comment on Statistical Methodology
We have begun trying to apply ideas discussed in Wasserstein, Schirm, and Lazar
(2019) about abandoning declarations of statstical significance. This landmark
editorial paper produced by the American Statistical Association discussed
various proposed ways to supplement p-values with additional statistics or
replace them altogether. We are still refining how to update our statistical
approaches in light of those ideas. 

One of the things we are doing is reporting Shannon information values
(s-values; Greenland, 2019; Wasserstein, Schirm, & Lazar, 2019). S-values are a
rescaling of p-values, such that $s = - log_2(p)$. They can range from $s = 0$
when $p = 1$ to $s = \infty$ when $p = 0$. Larger s-values correspond to greater
evidence against the null hypothesis. An s-value can be interpreted as how many
bits of information there are against the hypothesis. To make that easier to
understand, suppose you want to flip a coin repeatedly to determine whether it
is a fair coin rather than one biased toward landing on heads. Each flip
provides one bit of information, but only those resulting in heads are
information against the null hypothesis. A fair coin should yield heads half of
the time and tails the other half (independent with probability = .50 for each
outcome on each flip). The null hypothesis is that the coin is fair. Seeing 2
consecutive heads come up in a set of 2 flips ($p = 0.5^{2} = 0.25, s = 2$)
would be weak evidence against fairness but an s-value of 10 would be more
persuasive, like getting 10 heads from a set of 10 coin flips 
($p = 0.5^{10} = 0.0009765625, s = 10$).

Other ideas that look viable to use are reporting mimimum false positive risk
(mFPR; Colquhoun, 2009), Bayes Factor Bounds (BFB; Benjamin & Berger, 2019), and 
the upper bound for the posterior probability that that alternate hypothesis
(H1) is true (Benjamin & Berger, 2019). We have opted to use the latter two 
instead of mFPR. 

Taking the Can-Do self-assessment yields an ordinal score ranging from 1-5 that
represents the proficiency level of the learner who took the test. Higher levels
indicate greater proficiency. We can conceptualize the process that yields that
score as a sequential selection process comprised of a set of four level
transition tests (LTTs) that must be passed in strict order. Every learner
starts at level 1 and only advances to level 2 by passing the first LTT. The
learner's proficiency level is incremented for each LTT passed. The
self-assessment ends as soon as the learner fails to pass an LTT, or reaches the
maximum proficiency level (level 5). For a sample of $N$ learners, there will be
between $N$ and $4N$ binary LTT results (0 = fail, 1 = pass) depending on how 
many learners passed each LTT. 

We use continuation-ratio models to examine the proficiency levels achieved by 
the learners. These are simply logistic regression models applied to a 
reorganized dataset with one row of data per LTT attempted by each learner. 
We are usign a predictive modeling approach here that tests how well OPIc scores
and course predict self-assessed proficiency levels. Therefore, we evaluate the
models according to both discrimination and calibration criteria relevant to 
predictive models (Fenlon, O'Grady, Doherty, & Dunnion, 2018). 

\FloatBarrier

## Remaining Tasks

* Consider adding deviance plots based on Fenlon et al. (2018). 
* Streamline flow and eliminate unneeded output. 
* Recheck table object names and resulting table numbering in knitted output. 
* Review and update interpretation text and conclusions. 
* Add code to export figures that will be submitted with the manuscript. 


# SETUP
Set global R chunk options (local chunk options will over-ride global options). 

``` {r global_options}
# Global chunk options (over-ridden by local chunk options)
knitr::opts_chunk$set(include  = TRUE, echo = TRUE, error = TRUE, 
                      message = TRUE, warning = TRUE, fig.pos = "!ht")
```

\FloatBarrier

# LOAD PACKAGES & SET PACKAGE OPTIONS
Some of the functions we use below come from a personal R package called 
[piercer](https://github.com/sjpierce/piercer) that is available online from 
[GitHub](https://github.com). You can install it by changing the eval option in
next chunk from FALSE to TRUE, then running that chunk. You should only need to
do that once, unless you want to update the piercer package.

```{r install_piercer, eval = FALSE}
install.packages("devtools")
library(devtools)
install_github("sjpierce/piercer")
```

Load contributed R packages that we need to get additional functions. 

``` {r load_packages}
library(here)         # for here()
library(polycor)           # for hetcor()
library(car)               # for residualPlots(), influenceIndexPlot(), outlierTest()
library(multcomp)          # for glht()
library(visreg)            # for visreg()
# Set package options. 
# options(knitr.kable.NA = '0.00')
library(knitr)             # for kable()
library(texreg)            # for texreg()
library(pander)            # for pander()
library(pROC)              # for roc()
library(ggplot2)           # for ggplot()
library(tidyr)             # for unite()
library(dplyr)             # for filter(), select(), etc.
library(directlabels)      # for direct.label()
library(lattice)           # for strip.custom()
library(modEvA)            # for HLfit()
# Set package options. 
options(kableExtra.latex.load_packages = FALSE)
library(kableExtra)        # for kable_styling(), add_header_above(), 
                           # column_spec(), collapse_rows(), and landscape()
library(Hmisc)             # for rcorr()
library(piercer)           # for p2s(), p2bfb(), p2pp(), convertp(), lrcm(), 
                           # brier()
```

\FloatBarrier

# LOAD DATA

The file df_corr has the cleaned self-assesment data of 820 Spanish students,
one row per learner. The file df_crm comprises the same but expanded data with
one row per learner per level of self-assessed Spanish proficiency reached via
testing with Can-Do statements (see below for more information on the expanded
dataset). The data format of df_crm is conceptually similar to a person-period
file for longitudinal analysis. We need the expanded dataset (df_crm) to run
continuation-ratio models in the context of logistic regression. Other analyses
(i.e., correlational analyses) are performed based on the data frame of df_corr.

```{r}
df_corr <- read.csv(here::here("./data/df_corr.csv"))
df_crm  <- read.csv(here::here("./data/df_crm.csv"))
```

Now explore the structure of the two data frames, df_crm and df_corr. 

```{r}
# Explore the data structure of df_corr.
str(df_corr)
head(df_corr) 
dim(df_corr) # Rows, columns
```

Note that there is one row per learner in df_corr, with a total of 
`r nrow(df_corr)` rows. 

```{r}
# # Explore the data structure of df_crm.
str(df_crm)
head(df_crm)
dim(df_crm) # Rows, columns
```

Note that df_crm is an expanded data frame where there is one row per learner
per level transition attempted in the Can-Do self-assessment test. In total,
there are `r nrow(df_crm)` rows in df_crm.

\FloatBarrier

# DATA MANAGEMENT & EXPLORATION

Although the variable "Level" exists in both df_corr and df_crm, it has
different denotations in the two data frames. In df_corr, Level denotes the five
levels of Can-Do statements on the self-assessment, whereas in df_crm, Level
represents the transitions that exist between every two adjacent
self-assessment levels. The relationship between the two Level variables is
illustrated in this disgram: 1 --> 2 --> 3 --> 4 --> 5. Level in df_corr is
represented by the numbers, and Level in df_crm is represented by the arrows
connecting the numbers. In total, there are 4 transitions (Level in df_crm)
among the 5 levels (Level in df_corr) of can-do statments on the
self-assessment.

Unlike df_corr where there is one row per learner with Level indicating the
highest level a learner reached on the self-assessment, df_crm has been expanded
such that there is one row per learner per transition attempted. The binary
variable, pass, in df_crm indexes whether a learner passed/failed to pass a
given transition. For instance, for a learner who reached Level 3 on the
self-assessment, he would have one row in df_corr with Level equal to 3; in
contrast, the same learner would have three rows in df_crm, having a 1 on
"pass"" for the rows representing Transitions 1-2, and a 0 on "pass" for the row
representing Transition 3, or the highest transition that he/she attempted.

The data in df_crm is set up as described above for the application of
continuation-ratio modeling, which models the probabilities of passing the four
transitions (represented by Level in df_crm) as a function of predictors (which,
in the case of our study, are course and OPIc speaking scores). Each of those
transitions will eventually yield an estimate of the conditional pass rate,
which is the probability that an individual who reached the level on the left
end of the arrow succeeds in passing on to the level on the right end.
Continuation-ratio models can be conducted as logistic regressions where the
data frame has one row of data per person for each level transition that the
individual actually attempted.

We convert the variables of Course and Level in both data files from numeric to
factor variables to indicate that course level and self-assessment level are
ordinal variables.

``` {r }
# Convert Level and Course to factors
df_crm$Course<-as.factor(df_crm$Course)
df_crm$Level<-as.factor(df_crm$Level)
df_corr$Course<-as.factor(df_corr$Course)
df_corr$Level<-as.factor(df_corr$Level)
```

We treat OPIc speaking test proficiency scores as a continuous covariate that
ranges from Novice-low to Superior. The variable OPIc in df_corr shows the
actual OPIc ratings. Numerically transfomed OPIc scores are captured by OPIC in
df_corr (NL = 1, NM = 2, NH = 3, ..., AM = 8, AH = 9, Superior = 10). We removed
the cases for which the OPIc ratings were not meaningful for this study (AR, BR,
and UR). To make the model coefficients more interpretable, we included centered
OPIc scores instead of the original OPIc results in df_crm and stored them as
COPIC. A score of zero in COPIC represents the level of Intermediate-mid on the
OPIc scale.

\FloatBarrier

## Descriptive Statistics
Here we make some descriptive results reported in the manuscript reproducible. 
```{r}
# Frequency table for maximum level achieved. 
df_corr %>% 
   group_by(Level) %>% 
   summarise(N = n(),
             Pct = 100*N/nrow(df_corr)) %>% 
   ungroup() %>% 
   mutate(CumN = cumsum(N),
          CumPct = cumsum(Pct),
          RemainN = sum(N) - CumN,
          RemainPct = 100 - CumPct) ->
   T01

# Extract N and percent of learners with self-assessed level > 1 (i.e., >= 2). 
N_SA_GE_2   <- T01$RemainN[1]
Pct_SA_GE_2 <- T01$RemainPct[1]

# Extract N and percent of learners with self-assessed level > 2 (i.e., >= 3). 
N_SA_GE_3   <- T01$RemainN[2]
Pct_SA_GE_3 <- T01$RemainPct[2]

# Display formatted table.
kable(T01, format = "latex", booktabs = TRUE, digits = 0, 
      col.names = c("Level", "N", "%", "Cumulative N", "Cumulative %", 
                    "Remaining N", "Remaining %"),
      caption = "Frequency Table for Maximum Self-Assesment Level Achieved")
```

Only $N = `r N_SA_GE_2`$ (`r round(Pct_SA_GE_2, digits = 0)`%) of the learners 
reached level 2 or higher on the self-assessment. Only $N = `r N_SA_GE_3`$ 
(`r round(Pct_SA_GE_3, digits = 0)`%) learners reached level 3 or higher on the 
self-assessment. 

\FloatBarrier

## Crosstabulations (Learner Data)
We now examine the relationships among Level, Course, and OPIC by calling for
crosstabulations. First, we use the data frame df_corr.

```{r}
# Examine the relationship between SA level and course level. 
T02 <- addmargins(xtabs(~Level + Course, data = df_corr))
kable(T02, format = "latex", booktabs = TRUE, 
      caption = "Crosstab Self-Assessment Level by Course") %>%
kable_styling(latex_options = c("repeat_header"))
```

\newpage

```{r}
# Examine the relationship between OPIc scores and SA level.
T03a <- addmargins(xtabs(~Level + OPIC, data = df_corr))

# Examine the relationship between OPIc scores and course level.
T03b <- addmargins(xtabs(~Course + OPIC, data = df_corr))

# Combine two cross-tabs into a single object. 
T03  <- rbind(T03a, T03b) 

# Display formatted table. 
kable(T03, format = "latex", booktabs = TRUE,  
      caption = "Crosstabs of Self-Assessment Level and Course by OPIc Score") %>% 
kable_styling(latex_options = c("repeat_header")) %>% 
add_header_above(header = c(" ", "OPIc Score" = 9, " ")) %>% 
group_rows(group_label = "SA Level", bold = FALSE, start_row = 1, end_row = 6) %>% 
group_rows(group_label = "Course", bold = FALSE, start_row = 7, end_row = 11) %>% 
row_spec(row = c(6, 11), italic = TRUE) %>% 
column_spec(column = 11, italic = TRUE)
```

\FloatBarrier

## Crosstabulations (Transition Test Data)
We then examine some additional crosstabulations based on the data frame df_crm.
Note that Level in df_crm denotes the transitions between SA levels.

```{r}
# Crosstab to get a look at how often people pass each level transition.  
T04 <- addmargins(xtabs(~Level + pass, data = df_crm))
kable(T04, format = "latex", booktabs = TRUE, 
      caption = "Crosstab Self-Assessment Level Transition by Pass") %>%
kable_styling(latex_options = c("repeat_header")) %>% 
add_header_above(header = c(" ", "Pass" = 2, " ")) %>% 
group_rows(group_label = "Level", bold = FALSE, start_row = 1, 
           end_row = 5) %>% 
row_spec(row = 5, italic = TRUE) %>%
column_spec(column = 4, italic = TRUE)
```

```{r}
# Examine the relationship between SA level transition and course level.
T05 <- addmargins(xtabs(~Level + Course, data = df_crm))
kable(T05, format = "latex", booktabs = TRUE, 
      caption = "Crosstab Self-Assessment Level Transition by Course") %>%
kable_styling(latex_options = c("repeat_header")) %>% 
add_header_above(header = c(" ", "Course" = 4, " ")) %>% 
group_rows(group_label = "Level", bold = FALSE, start_row = 1, 
           end_row = 5) %>%
row_spec(row = 5, italic = TRUE) %>%
column_spec(column = 6, italic = TRUE)
```

```{r}
# Examine the relationship between centered OPIc scores and SA level transition.
T06 <- addmargins(xtabs(~Level + COPIC, data=df_crm)) 
kable(T06, format = "latex", booktabs = TRUE, 
      caption="Crosstab Self-Assessment Level Transition by OPIc rating") %>%
kable_styling(latex_options = c("repeat_header")) %>% 
add_header_above(header = c(" ", "Centered OPIc Score (OPIc - 5)" = 9, " ")) %>% 
group_rows(group_label = "Level", bold = FALSE, start_row = 1, 
           end_row = 5) %>%
row_spec(row = 5, italic = TRUE) %>%
column_spec(column = 11, italic = TRUE)
```

We then check the contrast coding associated with each of factors in df_crm so
we know how to interpret model coefficients later. The default for factors
should be dummy coding (treatment contrasts) with the first level set as the
reference group.

```{r}
# Check the default contrast coding for Level. Reference = "1". 
contrasts(df_crm$Level)
 
# Check the default contrast coding for Course. Reference = "100".
contrasts(df_crm$Course)
```

\FloatBarrier

# CORRELATIONS
We examine the correlations among course level, OPIc scores, and SA level using
the dataset df_corr (not in a long data format). As we mentioned earlier, Level
represents the highest level that a student reached on the SA. Polyserial
correlation is calculated for OPIC and Course and for OPIC and Level, whereas
polychoric correlation is calculated for the two ordinal variables, Course and
Level.

```{r}
# ovars = ordinal variables
ovars<-c("Level", "Course")

# cvars = continuous variables
cvar<-c("OPIC")

# Use the hetcor (heterogenous correlations) in "polycor" package to get a      
# heterogenous correlation matrix comprising Pearson correlations (if both      
# variables are continuous), polyserial correlations (if one variable is         
# continuous and the other one is ordinal), and polychoric correlations (if     
# both variables are ordinal).

HC <- hetcor(as.data.frame(df_corr[, c(cvar, ovars)]),
             ML=TRUE, use="pairwise.complete.obs")
T07 <- rbind(r.ps(HC, c("OPIC"), c("Level", "Course"), digits = 2, 
                  pdigits = NULL),
             r.pc(HC, c("Level", "Course"), digits = 2, pdigits = NULL))
kable(T07, format = "latex", booktabs = TRUE, 
      caption = "Correlations among OPIc, Course, and Level") %>%
kable_styling(latex_options = c("repeat_header"))
```

Notice that the Z-statistics are all really large, so the p-values are zero. 
That causes all the s-values to become infinite, and the BFB values to become
NaN (not-a-number) because we are dividing by zero in the denominator. You can
read more about interpreting s-values and BFB values in Greenland (2019),
Benjamin & Berger (2019), and Wasserstein, Schirm, & Lazar (2019). 

\FloatBarrier

# FIT CONTINUATION-RATIO MODELS
Here we fit a series of continuation-ratio models and display a combined table
summarizing them for comparison purposes. In subsequent sections, we examine 
each of the fitted models in detail. 

```{r fit_models}
# Model 1a: Parallel course effect
m1a <- glm(pass ~ Level + Course - 1, data = df_crm, family = "binomial")

# Model 1b: Non-parallel course effect
m1b <- glm(pass ~ Level + Course + Level*Course - 1, data = df_crm, 
           family = "binomial")

# Model 2a: Parallel OPIC effect
m2a <- glm(pass ~ Level + COPIC - 1, data = df_crm, family = "binomial")

# Model 2b: Non-parallel OPIC effect
m2b <- glm(pass ~ Level + COPIC + Level*COPIC - 1, data = df_crm, 
           family = "binomial")

# Model 3a: Non-parallel OPIC + parallel course effect
m3a <- glm(pass ~ Level + COPIC + Level:COPIC + Course - 1, data = df_crm, 
           family = "binomial")

# Create a list of the model fit objects.
TModels <- list(m1a, m1b, m2a, m2b, m3a)
```

The table below shows the model parameters for the whole set of models.  

\begin{landscape}

```{r display_models, results='asis'} 

texreg(TModels, use.packages = FALSE, dcolumn = TRUE, booktabs = TRUE, 
       single.row = TRUE, digits = 3, label = "TModels", 
       custom.model.names = c("Model 1a",  "Model 1b", "Model 2a", "Model 2b",
                              "Model 3a"), 
       caption = "Continuation-Ratio Models")
```

\end{landscape}

\FloatBarrier

# MODEL 1A: PARALLEL COURSE EFFECT
We fit a series of continuation-ratio models to df_crm to see whether course
level and OPIc scores significantly predict the pass rate for each transition on
the SA. We first fit a basic model that omits the intercept term in order to
simplify post-processing of the model results into interpretable estimates. We
focus here on a model examining whether the course a learner is taking affects
the pass rate for each level transition. This model assumes that the higher
level courses have a constant effect on the pass rates across all level
transitions. The summary below shows the actual model fit information and the
raw parameter estimates.

\newpage

```{r}
T08 <- summary(m1a)
panderOptions('keep.trailing.zeros', TRUE)
set.caption("Model 1a Summary")
pander(T08, digits = c(5, 5, 4, 3), round = c(4, 4, 2, Inf), justify = "right",
       caption = "Model 1a Summary")
```

Post-process the raw model output to get additional information, including 
s-values, BFBs, and posterior probabilities of H1 corresponding to the p-values 
listed above.

```{r}
format(convertp(T08$coefficients[,"Pr(>|z|)"], digits = 2), digits = 3)
```

\FloatBarrier

## Sequential Tests (Type I SS)
Each row in Table 10 tests the significance of unique additional variance
explained by the term on that line after controlling for all previously entered
terms. Significant results mean adding that term improved the model.

\newpage

```{r}
T10 <- anova(m1a, test = "Chisq")
T10 <- cbind(T10, convertp(T10[,"Pr(>Chi)"]))
pander(T10, justify = "right", split.tables = Inf, 
       digits = c(5, 5, 5, 8, 3, 4, 3, 3), 
       round = c(0, 3, 0, 3, Inf, 2, 2, 2), 
       caption = "Model 1a Sequential Tests (Type I SS): Analysis of Deviance")
```

\FloatBarrier

## Simultaneous Tests of Main Effects via LRT (Type III SS)
The simultaneous tests in Table 11 are the effects of the indicated terms after
controlling for all other terms in the model. They are only computed for terms
that are not part of a higher-order interaction because it makes no sense to
test for a main effect when the variable is involved in an interaction. These
should be functionally equivalent to the results you get with *anova()* when you
feed it a pair of nested models that differ only in that one model includes a
term that is absent from the other model. They are likelihood ratio tests
(LRTs).

```{r}
T11 <- drop1(m1a, test = "Chisq")   
T11 <- cbind(T11, convertp(T11[,"Pr(>Chi)"]))
pander(T11, justify = "right", split.tables = Inf, 
       digits = c(5, 8, 8, 8, 3, 4, 3, 3),
       round = c(0, 3, 3, 3, Inf, 2, 2, 2), 
       caption = "Model 1a Simultaneous Tests (Type III SS)")
```

\FloatBarrier

## Conditional and Unconditional Pass Rates
We use the inverse logit transformation to convert fitted values and associated
confidence intervals into the conditional probability of passing a particular
level transition given that the student is in a particular Spanish course.

We compute the unconditional pass rates from the conditional pass rates as sets
of cumulative products.

```{r}
# Create a new data frame object for use with predict()
m1a.ND <- data.frame(Level = gl(n = 4, k = 1, length = 16,
                               labels = c("1", "2", "3", "4")), 
                    Course = gl(n = 4, k = 4, length = 16, 
                                labels = c("100", "200", "300", "400")))

# Compute predicted mean passing rate at each combination of Level & Course
m1a.pred <- predict(m1a, newdata = m1a.ND, type = "link", se.fit = TRUE)

# Add fitted values and CIs to the new data frame & display it.
critval       <- qnorm(0.975)  # For Wald 95% CIs
m1a.ND$fit    <- m1a.pred$fit
m1a.ND$se.fit <- m1a.pred$se.fit
m1a.ND$fit.LL <- with(m1a.ND, fit - (critval * se.fit))
m1a.ND$fit.UL <- with(m1a.ND, fit + (critval * se.fit))

# Convert fitted values and CIs to probabilities. 
m1a.ND$Pass.Rate <- invlogit(m1a.ND$fit)
m1a.ND$Pass.LL   <- invlogit(m1a.ND$fit.LL)
m1a.ND$Pass.UL   <- invlogit(m1a.ND$fit.UL)

# Compute unconditional pass rates.
m1a.ND$Pass.URate <- c(cumprod(m1a.ND[m1a.ND$Course == "100", "Pass.Rate"]),
                       cumprod(m1a.ND[m1a.ND$Course == "200", "Pass.Rate"]),
                       cumprod(m1a.ND[m1a.ND$Course == "300", "Pass.Rate"]),
                       cumprod(m1a.ND[m1a.ND$Course == "400", "Pass.Rate"]))

ShowVars <- c("Level", "Course", "Pass.Rate", "Pass.LL", "Pass.UL", "Pass.URate")
T12 <- m1a.ND[, ShowVars] 
kable(T12, format = "latex", booktabs = TRUE, digits = 2, 
      format.args = list(nsmall = 2),
      caption = paste("Conditional Pass Rates with 95 percent CIs and",
                      "Unconditional Pass Rates by Level Transition and Course")) %>%
kable_styling(latex_options = c("repeat_header"))
```

\FloatBarrier

## Odds-Ratios for Course Effect
We exponentiate the course parameters to obtain the odds-ratios showing the
effect of being in a second-, third-, or fourth-year Spanish course instead of a
first-year course. This quantifies the effect of course, which this model
assumes is equal across level transitions. We will test whether that assumption
is reasonable in another model later. We are using contrasts to do pairwise
comparisons and get simultaneous 95% CIs that are both adjusted for multiple
testing via Westfall's (1997) method.

```{r}
# Run multiple comparisons examine the course effect & get adjusted 95% CIs. 
m1a.ct <- glht(m1a, linfct = mcp(Course = "Tukey"))
m1a.mc <- summary(m1a.ct, test = adjusted("Westfall"))
m1a.ci <- confint(m1a.ct, calpha = adjusted_calpha(test = "Westfall"))
T13 <- data.frame(Est   = m1a.mc$test$coefficients, 
                  SE    = m1a.mc$test$sigma, 
                  CI.LL = m1a.ci$confint[, "lwr"],
                  CI.UL = m1a.ci$confint[, "upr"],
                  OR    = exp(m1a.mc$test$coefficients),
                  OR.LL = exp(m1a.ci$confint[, "lwr"]),
                  OR.UL = exp(m1a.ci$confint[, "upr"]),
                  z     = m1a.mc$test$tstat,
                  p     = m1a.mc$test$pvalues,
                  Sval  = p2s(m1a.mc$test$pvalues),
                  BFB   = p2bfb(m1a.mc$test$pvalues),
                  PPH1  = p2pp(m1a.mc$test$pvalues))
kable(T13, format = "latex", booktabs = TRUE, format.args = list(digits = 3),
      digits = c(rep(x = 2, times = 8), Inf, 2, 2, 2), 
      caption = paste("Multiple Comparisons for Course Effect, Westfall (1997)",
      "Adjustment for Multiplicity")) %>%
kable_styling(latex_options = c("repeat_header"))
```

\FloatBarrier

## Assessing Goodness of Fit, Discrimination, and Calibration
Since we are running the continuation-ration model as a logistic regression, all
the stadard methods for assessing goodness of fit for logistic regresion models
apply.

\FloatBarrier

### Hosmer-Lemeshow goodness of fit test. 
The null hypothesis for the Hosmer-Lemeshow test (HLT) is that the data fit the model,
so a significant result is actually undesirable because it says that the data
don't fit the model. This is a measure of calibration (Fenlon et al., 2018), 
which can also be examined with a calibration plot like the one shown below.

The calibration plot uses the same set of bins used for the HLT. We used a
binning method that creates bins by grouping the predicted probabilities into
bins with a minimum width of 0.10 because that way all the values grouped
together are quite similar. The confidence interval for each bin is labeled with
the sample size for the bin. Ideally, the point estimate for each bin is close
to the dashed line, but at a minimum we want all the confidence intervals to
overlap that dashed line.

```{r}
HLT.m1a <- HLfit(m1a, bin.method = "prob.bins", min.prob.interval = 0.1, 
                 main = "Calibration Plot", 
                 xlab = "Predicted Probability (minimum bin interval = 0.10)",
                 ylab = "Observed Probability")
HLT.m1a
```

```{r}
# Get the s-value, BFB, and posterior probability of H1. 
convertp(p = HLT.m1a$p.value, digits = 2)
```

\FloatBarrier

### Brier Score
The Brier score is an overall measure of accuracy suitable for logistic 
regression models with binary outcomes (Fenlon et al., 2018). It is the average
prediction error (Steyerberg et al., 2001; Steyerberg et al., 2010). The scaled 
Brier score adjusts for unscaled version to have a range of from 0 to 1, thereby
making it similar to an $R^{2}$ statistic. Values close to 1 are desirable and 
indicate good calibration. We report the scaled Brier score below.

```{r}
brier(m1a, digits = 2)
```

\FloatBarrier

### Classification 
Below, we use functions from the pROC package to identify the optimal cutpoint 
$c$ to use when converting fitted probabilities of passing a LTT into a binary
prediction of whether the individual will pass it. The naieve cutpoint would be
$c = 0.5$, but that is not necessarily the optimal cutpoint. Choosing a cutpoint
via use of the Youden index (Youden, 1950) is optimal with respect to overall
misclassification rate for a given weighting of sensitivity and specificity
(Perkins & Schisterman, 2006). Below, we show the results of various 
classification measures given that we use the optimal cutpoint based on the 
Youden index. 

```{r}
df_crm$pred.m1a <- predict(m1a, type = "response")
set.seed(4921) # For reproducibility of bootstrap estimates.
roc.m1a <- roc(pass ~ pred.m1a, data = df_crm, ci = TRUE, direction = "<",
               ci.method = "bootstrap", boot.n = 10000)
round(lrcm(roc.m1a, seed = 4684), digits = 3)
```

Here are short definitions of what some of these statistics measure. 

* Accuracy is the model's overall ability to correctly classify learners
  according to whether they pass vs fail the level transitions. 
* Sensitivity is the ability of the model to correctly identify learners who 
  passed level transitions. 
* Specificity is the ability of the model to correctly identify learners who 
  fail the level transitions. 

\FloatBarrier

### Area Under the Curve (AUC) 
The area under the receiver operating characteristic curve (AUC) measures the
model's ability to discriminate those who pass from those who fail to pass the
level transitions. It can range from 0.50 to 1.00; values of 0.50--0.69 are
poor, 0.70-0.79 are acceptable, .80-.89 are excellent, and $\geq$ 0.90 are
outstanding. These interpretive heuristics come from Hosmer, Lemeshow, &
Sturdivant (2013, p. 177).

```{r}
print(roc.m1a)
plot.roc(roc.m1a, print.auc = TRUE, main = "ROC Curve, Model 1a", 
         print.thres = "best")
```

Based on this AUC result, we conclude that model 1a is generally poor at
discriminating people who are passing vs failing the level transitions. That
means course is not a particularly good predictor regardless of what hypothesis
testing results show.

\FloatBarrier

### $R^{2}$ Measures
Pseudo-$R^{2}$ ($R^{2}_p$) is the squared Pearson correlation between observed
and predicted values, as suggested in Hosmer, Lemeshow, & Sturdivant (2013, p.
182). Meanwhile, $R^{2}_{Dev}$ is a measure based on deviance residuals (Fox,
1997, p. 451; Cameron & Windmeijer, 1997).

```{r}
c(PseudoR2 = pseudoR2(m1a, digits = 2), R2Dev = R2Dev(m1a, digits = 2))
```

The low $R^{2}$ is not very encouraging about this model. 

\FloatBarrier

## Diagnostics   
We need to run model diagnostics to see if there are any obvious problems with
the model. First we check for outliers and find that there are none of note.

```{r}
outlierTest(m1a)
```

Now we look at some residual plots.
```{r}
residualPlots(m1a, layout = c(1,3))
```

Now we look at some index plots for influence measures. We are looking for
observations with values exceeding the cutoffs.

```{r}
# Plot Cook's Distance vs observation index.
PlotCookD(m1a)

# How many cases have high Cook's Distances (D > cutoff)?
summary(cooks.distance(m1a))
addmargins(table(cooks.distance(m1a) > CookDco(m1a)))

# Plot Leverage (hat) values vs. observation index.
PlotHat(m1a)

# Plot Standardized Pearson Residuals vs Leverage, w/ Cook's D contours.
plot(m1a, which = 5, id.n = 10, cex.id = .6, 
     cook.levels = round(CookDco(m1a), digits = 3))
abline(v = hatco(m1a), lty = 2, col = "blue")
text(x = hatco(m1a), y = 3.3, pos = 4, col = "blue", cex = .75,
     labels = paste("Leverage cutoff >", round(hatco(m1a), digits = 3)))

# How many cases have high leverage (hat > cutoff)?
summary(hatvalues(m1a))
addmargins(table(hatvalues(m1a) > hatco(m1a)))

# Identify cases w/ high leverage and high Cook's D. 
InfCases(m1a)
```

There are a lot of observations with high Cook's distance and high leverage.
That is a bit concerning, but this may not be the final model we want to use for
drawing conclusions. 

\FloatBarrier

## Graphs

\FloatBarrier

### Main Effects

```{r}
visreg(m1a, ylab = "Log odds (Pass)")
visreg(m1a, ylab = "Conditional Pass Rate", scale = "response", rug = 2)
```

\FloatBarrier

### Conditional Pass Rates
Use ggplot to generate the graph for conditional pass rates derived from model
m1a. 

```{r}
F1 <- ggplot(T12, aes(Level, Pass.Rate, group=Course, color=Course))+
      geom_line()+ coord_cartesian(ylim = c(0, 0.8))+
      xlab("SA Level Transition")+
      ylab("Conditional Pass Rate")+
      ggtitle(label="Conditional Pass Rates by Course and SA Level Transition")
direct.label(F1, "first.polygons")
```

\FloatBarrier

### Unconditional Pass Rates
Use ggplot to generate the graph for unconditional pass rates derived from model
m1a. 

```{r}
F2 <- ggplot(T12, aes(Level, Pass.URate, group=Course, color=Course))+
      geom_line()+ coord_cartesian(ylim = c(0, 0.65))+
      xlab("SA Level Transition")+
      ylab("Unconditional Pass Rate")+
      ggtitle(label="Unconditional Pass Rates by Course and SA Level Transition")
direct.label(F2, "first.polygons")
```

\FloatBarrier

## Conclusion
Learners taking second-, third-, and fourth-year courses are more likely to pass
each of the level transitions than learners in the first-year courses. The
increasing trend in the odds-ratios across the more advanced courses indicate
that the more advanced the course, the larger the difference in the passing
rates is relative to first-year courses. This makes intuitive sense: we expect
learners with more training to do better on self-assessments if the instrument
is a valid measure of Spanish proficiency.

The course effects in this model are assumed to be parallel (constant across the
different level transitions), but we can test whether that assumption needs to
be relaxed by running another model and comparing it to this one.

Despite the significant effects, the model doesn't discriminate well between
those who pass vs. fail (it has modest accuracy, low sensitivity, high
specificity, and low AUC). It explains only a small percentage of the variance.
We can probably do better by considering another predictor. 

\FloatBarrier

# MODEL 1B: NON-PARALLEL COURSE EFFECT
We again omit the intercept term in order to simplify post-processing of the
model results into interpretable estimates. We continue to focus on a model
examining whether the course a learner is taking affects the pass rate, but
relax the assumption that it has a constant effect across all level transitions.
This model allows course to have a level-specific effect on the pass rates. The
summary below shows the actual model fit information and the raw parameter
estimates.

```{r}
T14 <- summary(m1b)
set.caption("Model 1b Summary")
pander(T14, digits = c(5, 5, 4, 3), round = c(4, 4, 2, Inf), justify = "right",
       caption = "Model 1b Summary")
```

Post-process the raw model output to get additional information, including 
s-values, BFBs, and posterior probabilities of H1.

```{r}
format(convertp(T14$coefficients[,"Pr(>|z|)"], digits = 2), digits = 3)
```

Let's show models 1a and 1b side by side. 

```{r, results='asis'} 
T16 <- list(m1a, m1b)
texreg(T16, use.packages = FALSE, dcolumn = TRUE, booktabs = TRUE, 
       single.row = TRUE, digits = 3, label = "T16", 
       custom.model.names = c("Model 1a Parallel",  "Model 1b Non-Parallel"), 
       caption = "Parallel vs. Non-Parallel Course Effect")
```

\newpage

\FloatBarrier

## Sequential Tests (Type I SS)
Here, we want to focus on the result for the interaction effect. That tells us
whether allowing course to have a level-specific effect instead of a constant
effect across levels improved the model.

```{r}
T17 <- anova(m1b, test = "Chisq")
T17 <- cbind(T17, convertp(T17[,"Pr(>Chi)"]))
pander(T17, justify = "right", split.tables = Inf, 
       digits = c(5, 5, 5, 8, 3, 4, 3, 3), 
       round = c(0, 3, 0, 3, Inf, 2, 2, 2), 
       caption = "Model 1b Sequential Tests (Type I SS): Analysis of Deviance")
```

\FloatBarrier

## Simultaneous Tests of Interaction Effects via LRT (Type III SS)

Another way to test whether the interaction was significant is to compare models
via a likelihood ratio test (LRT). We can do that by feeding the *anova()*
function two model fit objects. The models must be nested (one contains only a
subset of the parameters in the other model).

\newpage

```{r}
#CONTINUE  HERE
T18 <- anova(m1a, m1b, test = "Chisq")
T18 <- cbind(T18, convertp(T18[,"Pr(>Chi)"]))
pander(T18, justify = "right", split.tables = Inf, 
       digits = c(5, 8, 5, 8, 3, 4, 3, 3), 
       round = c(0, 3, 0, 3, Inf, 2, 2, 2),
       caption = "Model 1a vs 1b: Likelihood Ratio Test for Interaction")
```

Both methods demonstrated for testing the interaction term yield the same
p-value, so we will only use one of them from here on out.

\FloatBarrier

## Conditional and Unconditional Pass Rates
We omit these rates because model 1b is not better than model 1a. 

\FloatBarrier

## Odds-Ratios for Course Effect
We omit these odds-ratios because model 1b is not better than model 1a. 

\FloatBarrier

## Assessing Goodness of Fit, Discrimination, and Calibration
Since we are running the continuation-ration model as a logistic regression, all
the stadard methods for assessing goodness of fit for logistic regresion models
apply.

\FloatBarrier

### Hosmer-Lemeshow goodness of fit test. 
The null hypothesis for the Hosmer-Lemeshow test is that the data fit the model,
so a significant result is actually undesirable because it says that the data
don't fit the model. This is a measure of calibration (Fenlon et al., 2018), 
which can also be examined with a calibration plot like the one shown below.

The calibration plot uses the same set of bins used for the HLT. We used a
binning method that creates bins by grouping the predicted probabilities into
bins with a minimum width of 0.10 because that way all the values grouped
together are quite similar.

```{r}
HLT.m1b <- HLfit(m1b, bin.method = "prob.bins", min.prob.interval = 0.1, 
                 main = "Calibration Plot", 
                 xlab = "Predicted Probability (minimum bin interval = 0.10)",
                 ylab = "Observed Probability")
HLT.m1b
```

```{r}
# Get the s-value, BFB, and posterior probability of H1. 
convertp(p = HLT.m1b$p.value, digits = 2)
```

\FloatBarrier

### Brier Score
The unscaled Brier score is an overall measure of accuracy suitable for logistic
regression models with binary outcomes (Fenlon et al., 2018). It is the average
prediction error (Steyerberg et al., 2001; Steyerberg et al., 2010). The scaled 
Brier score adjusts for unscaled version to have a range of from 0 to 1, thereby
making it similar to an $R^{2}$ statistic. We report the scaled Brier score 
below.  

```{r}
brier(m1b, digits = 2)
```

\FloatBarrier

### Classification 
Here we want to examine the classification table, plus the overall accuracy,
sensitivity, and specificity estimates.

```{r}
df_crm$pred.m1b <- predict(m1b, type = "response")
set.seed(1574) # For reproducible bootstrap estimates.
roc.m1b <- roc(pass ~ pred.m1b, data = df_crm, ci = TRUE, direction = "<",
               ci.method = "bootstrap", boot.n = 10000)
round(lrcm(roc.m1b, seed = 6711), digits = 3)
```

The low sensitivity indicates that the model does a poor job of correctly
identifying those who pass the level transitions. The high specificity indicates
that it does a good job of identifying those who fail to pass level transitions.

\FloatBarrier

### Area Under the Curve (AUC)
The area under the receiver operating characteristic curve (AUC) measures the
model's ability to discriminate those who pass from those who fail to pass the
level transitions. It can range from 0.50 to 1.00; values of 0.50--0.69 are
poor, 0.70-0.79 are acceptable, .80-.89 are excellent, and $\geq$ 0.90 are
outstanding.

```{r}
print(roc.m1b)
plot.roc(roc.m1b, print.auc = TRUE, main = "ROC Curve, Model 1b", 
         print.thres = "best")
```

Based on this AUC result, we conclude that model 1b does a poor job of
discriminating between those who pass vs fail the level transitions. That means
course is still not a particularly good predictor.

Next we compare the ROC curves for models m1a and m1b using a bootstrap test for
difference between the AUC values (Robin et al., 2011). 

```{r}
set.seed(9537) # For reproducible bootstrap estimates.
roct.m1a.m1b <- roc.test(roc.m1a, roc.m1b, method = "bootstrap", boot.n = 10000)
roct.m1a.m1b
```

```{r}
# Get the s-value, BFB, and posterior probability of H1. 
convertp(p = roct.m1a.m1b$p.value, digits = 2)
```

\FloatBarrier

### $R^{2}$ Measures
Pseudo-$R^{2}$ ($R^{2}_p$) is the squared Pearson correlation between observed
and predicted values, as suggested in Hosmer, Lemeshow, & Sturdivant (2013, p.
182). Meanwhile, $R^{2}_{Dev}$ is a measure based on deviance residuals (Fox,
1997, p. 451; Cameron & Windmeijer, 1997). The result for model 1b is no better
than it was for model 1a.

```{r}
c(PseudoR2 = pseudoR2(m1b, digits = 2), R2Dev = R2Dev(m1b, digits = 2))
```

\FloatBarrier

## Diagnostics
We omit detailed diagnostics because model 1b is not better than model 1a. 

\FloatBarrier

## Graphs
We omit graphs because model 1b is not better than model 1a. See model 1a graphs
instead.

\FloatBarrier

## Conclusion
The interaction is not improving the model. We should interpret model 1a instead
of model 1b. That may change if we build in other covariates. However, we should
also test COPIC as an alternative to course. We turn to that next in model 2a.

\FloatBarrier

# MODEL 2A: PARALLEL OPIC EFFECT
We now want to look at a different covariate, namely OPIc speaking proficiency
scores. We again fit a model that omits the intercept term in order to simplify
post-processing of the model results into interpretable estimates. We start by
assuming that the OPIc scores have a constant effect on the pass rates across
all level transitions. We are using the centered version of OPIc in the model.
The summary below shows the actual model fit information and the raw parameter
estimates.

```{r}
T19 <- summary(m2a)
set.caption("Model 2a Summary")
pander(T19, digits = c(5, 5, 4, 3), round = c(4, 4, 2, Inf), justify = "right",
       caption = "Model 2a Summary")
```

Post-process the raw model output to get additional information, including 
s-values, BFBs, and posterior probabilities of H1.

```{r}
format(convertp(T19$coefficients[,"Pr(>|z|)"], digits = 2), digits = 3)
```

\FloatBarrier

# Sequential Tests (Type I SS)
Each row in Table 21 tests the significance of unique additional variance
explained by the term on that line after controlling for all previously entered
terms. Significant results mean adding that term improved the model.

```{r}
T21 <- anova(m2a, test = "Chisq")
T21 <- cbind(T21, convertp(T21[,"Pr(>Chi)"]))
pander(T21, justify = "right", split.tables = Inf, 
       digits = c(5, 5, 5, 8, 3, 4, 3, 3), 
       round = c(0, 3, 0, 3, Inf, 2, 2, 2),
       caption = "Model 2a Sequential Tests (Type I SS): Analysis of Deviance")
```

\FloatBarrier

## Simultaneous Tests of Main Effects via LRT (Type III SS)
The simultaneous tests in Table 22 are the effects of the indicated terms after
controlling for all other terms in the model. They are only computed for terms
that are not part of a higher-order interaction because it makes no sense to
test for a main effect when the variable is involved in an interaction. These
should be functionally equivalent to the results you get with *anova()* when you
feed it a pair of nested models that differ only in that one model includes a
term that is absent from the other model. They are likelihood ratio tests
(LRTs).

```{r}
T22 <- drop1(m2a, test = "Chisq") 
T22 <- cbind(T22, convertp(T22[,"Pr(>Chi)"]))
pander(T22, justify = "right", split.tables = Inf, 
       digits = c(5, 8, 8, 8, 3, 4, 3, 3),
       round = c(0, 3, 3, 3, Inf, 2, 2, 2), 
       caption = "Model 2a Simultaneous Tests (Type III SS)")
```

\FloatBarrier

## Conditional and Unconditional Pass Rates
We omit these rates because model 2b is better than model 2a. See the results
for model 2b instead.

\FloatBarrier

## Odds-Ratio for COPIC Effect 
We exponentiate the COPIC parameter to obtain the odds-ratio showing the effect
of an extra point on the COPIC (i.e., a score of 6 vs 5 in the raw variable).
This quantifies the effect of COPIC, which this model assumes is equal across
level transitions. We will test whether that assumption is reasonable in another
model later.

```{r}
T23   <- cbind(OR = exp(coef(m2a)[5]), 
               LL = exp(confint(m2a, level = 0.95)[5, 1]),
               UL = exp(confint(m2a, level = 0.95)[5, 2]))
dimnames(T23)[[1]] <- "COPIC (0 vs 1)"
kable(T23, format = "latex", booktabs = TRUE, digits = 2, 
      caption = paste("Odds-Ratios for COPIC Effect (Estimates and 95 Percent",
                      "Confidence Intervals)")) %>% 
kable_styling(latex_options = c("repeat_header")) 
```

\FloatBarrier

## Assessing Goodness of Fit, Discrimination, and Calibration
Since we are running the continuation-ration model as a logistic regression, all
the stadard methods for assessing goodness of fit for logistic regresion models
apply.

\FloatBarrier

### Hosmer-Lemeshow goodness of fit test. 
The null hypothesis for the Hosmer-Lemeshow test is that the data fit the model,
so a significant result is actually undesirable because it says that the data
don't fit the model. This is a measure of calibration (Fenlon et al., 2018), 
which can also be examined with a calibration plot like the one shown below.

The calibration plot uses the same set of bins used for the HLT. We used a
binning method that creates bins by grouping the predicted probabilities into
bins with a minimum width of 0.10 because that way all the values grouped
together are quite similar.

```{r}
HLT.m2a <- HLfit(m2a, bin.method = "prob.bins", min.prob.interval = 0.1, 
                 main = "Calibration Plot", 
                 xlab = "Predicted Probability (minimum bin interval = 0.10)",
                 ylab = "Observed Probability")
HLT.m2a
```

```{r}
# Get the s-value, BFB, and posterior probability of H1. 
convertp(p = HLT.m2a$p.value, digits = 2)
```

\FloatBarrier

### Brier Score
The unscaled Brier score is an overall measure of accuracy suitable for logistic
regression models with binary outcomes (Fenlon et al., 2018). It is the average
prediction error (Steyerberg et al., 2001; Steyerberg et al., 2010). The scaled 
Brier score adjusts for unscaled version to have a range of from 0 to 1, thereby
making it similar to an $R^{2}$ statistic. We report the scaled Brier score 
below.  

```{r}
brier(m2a, digits = 2)
```

\FloatBarrier

### Classification 
Here we want to examine the classification table, plus the overall accuracy,
sensitivity, and specificity estimates.

```{r}
df_crm$pred.m2a <- predict(m2a, type = "response")
set.seed(5378) # For reproducible bootstrap estimates.
roc.m2a <- roc(pass ~ pred.m2a, data = df_crm, ci = TRUE, direction = "<",
               ci.method = "bootstrap", boot.n = 10000)
round(lrcm(roc.m2a, seed = 9825), digits = 3)
```

\FloatBarrier

### Area Under the Curve (AUC)
The area under the receiver operating characteristic curve (AUC) measures the
model's ability to discriminate those who pass from those who fail to pass the
level transitions. It can range from 0.50 to 1.00; values of 0.50--0.69 are
poor, 0.70-0.79 are acceptable, .80-.89 are excellent, and $\geq$ 0.90 are
outstanding.

```{r}
print(roc.m2a)
plot.roc(roc.m2a, print.auc = TRUE, main = "ROC Curve, Model 2a", 
         print.thres = "best")
```

Next we compare the ROC curves for models m1a and m2a using a bootstrap test for
difference between the AUC values (Robin et al., 2011). 

```{r}
set.seed(7436) # For reproducible bootstrap estimates.
roct.m1a.m2a <- roc.test(roc.m1a, roc.m2a, method = "bootstrap", boot.n = 10000)
roct.m1a.m2a
```

```{r}
# Get the s-value, BFB, and posterior probability of H1. 
convertp(p = roct.m1a.m2a$p.value, digits = 2)
```

\FloatBarrier

### $R^{2}$ Measures
Pseudo-$R^{2}$ ($R^{2}_p$) is the squared Pearson correlation between observed
and predicted values, as suggested in Hosmer, Lemeshow, & Sturdivant (2013, p.
182). Meanwhile, $R^{2}_{Dev}$ is a measure based on deviance residuals (Fox,
1997, p. 451; Cameron & Windmeijer, 1997).

```{r}
c(PseudoR2 = pseudoR2(m2a, digits = 2), R2Dev = R2Dev(m2a, digits = 2))
```

\FloatBarrier

## Diagnostics
We omit model diagnostics because model 2b is better than model 2a.

\FloatBarrier

## Graphs
We omit graphs because model 2b is better than model 2a. See the graphs for
model 2b instead.

\FloatBarrier

## Conclusion
Model 2a suggests that higher OPIC scores are associated with higher passing
rates across the level transitions. Furthermore, model 2a is clearly better than
model 1a. However, we still need to run another model to see whether relaxing
the assumption that the OPIC effect is constant across levels is viable. We do
that in model 2b.

\newpage

\FloatBarrier

# MODEL 2B: NON-PARALLEL OPIC EFFECT
We now relax the assumption that the OPIC effect is constant across level
transitions. We again fit a model that omits the intercept term in order to
simplify post-processing of the model results into interpretable estimates. We
are still using the centered version of OPIC in the model. The summary below
shows the actual model fit information and the raw parameter estimates.

```{r}
T24 <- summary(m2b)
set.caption("Model 2b Summary")
pander(T24, digits = c(5, 5, 4, 3), round = c(4, 4, 2, Inf), justify = "right",
       caption = "Model 2b Summary")
```

Post-process the raw model output to get additional information, including 
s-values, BFBs, and posterior probabilities of H1.

```{r}
format(convertp(T24$coefficients[,"Pr(>|z|)"], digits = 2), digits = 3)
```

Let's show the models 2a and 2b side by side. 

```{r, results='asis'} 
T26 <- list(m2a, m2b)
texreg(T26, use.packages = FALSE, dcolumn = TRUE, booktabs = TRUE, 
       single.row = TRUE, digits = 3, label = "T26", 
       custom.model.names = c("Model 2a Parallel", "Model 2b Non-Parallel"), 
       caption = "Parallel vs. Non-Parallel OPIC Effect")
```

\newpage

We can see that the difference in AICs
($\Delta AIC = AIC_{2a} - AIC_{2b} = `r round(AIC(m2a) - AIC(m2b), digits = 3)`$)
favors model 2b, which has the lower AIC value, but not by a large margin. 
Meanwhile, difference in BICs 
($\Delta BIC = BIC_{2a} - BIC_{2b} = `r round(BIC(m2a) - BIC(m2b), digits = 3)`$) 
favors model 2a by a larger margin because it imposes a stronger penalty for model complexity 
(lack of parsimony). 

\FloatBarrier

## Sequential Tests (Type I SS)
Each row in Table 27 tests the significance of unique additional variance
explained by the term on that line after controlling for all previously entered
terms. Significant results mean adding that term improved the model. Here, we
want to focus on the result for the interaction effect. That tells us whether
allowing OPIc to have a level-specific effect instead of a constant effect
across levels improved the model.

```{r}
T27 <- anova(m2b, test = "Chisq")
T27 <- cbind(T27, convertp(T27[,"Pr(>Chi)"]))
pander(T27, justify = "right", split.tables = Inf, 
       digits = c(5, 5, 5, 8, 3, 4, 3, 3), 
       round = c(0, 3, 0, 3, Inf, 2, 2, 2), 
       caption = "Model 2b Sequential Tests (Type I SS): Analysis of Deviance")
```

\newpage

\FloatBarrier

## Simultaneous Tests of Interaction Effects via LRT (Type III SS)
Another way to test whether the interaction was significant is to compare models
via a likelihood ratio test (LRT). We can do that by feeding the *anova()*
function two model fit objects. The models must be nested (one contains only a
subset of the parameters in the other model).

```{r}
T28 <- anova(m2a, m2b, test = "Chisq")   
T28 <- cbind(T28, convertp(T28[,"Pr(>Chi)"]))
pander(T28, justify = "right", split.tables = Inf, 
       digits = c(5, 8, 5, 8, 3, 4, 3, 3), 
       round = c(0, 3, 0, 3, Inf, 2, 2, 2),
       caption = "Model 2b Simultaneous Tests (Type III SS)")
```

\FloatBarrier

## Conditional and Unconditional Pass Rates
We use the inverse logit transformation to convert fitted values and associated
confidence intervals into the conditional probability of passing a particular
level transition given that the student has a particular COPIC score.

We compute the unconditional pass rates from the conditional pass rates as sets
of cumulative products.

```{r}
# Create a new data frame object for use with predict()
m2b.ND <- data.frame(Level = gl(n = 4, k = 1, length = 36,
                               labels = c("1", "2", "3", "4")), 
                     COPIC = rep(-4:4, each = 4),
                     OPIC = rep(1:9, each = 4))

# Compute predicted mean passing rate at each combination of Level & Course
m2b.pred <- predict(m2b, newdata = m2b.ND, type = "link", se.fit = TRUE)

# Add fitted values and CIs to the new data frame & display it.
critval      <- qnorm(0.975)  # For Wald 95% CIs
m2b.ND$fit    <- m2b.pred$fit
m2b.ND$se.fit <- m2b.pred$se.fit
m2b.ND$fit.LL <- with(m2b.ND, fit - (critval * se.fit))
m2b.ND$fit.UL <- with(m2b.ND, fit + (critval * se.fit))

# Convert fitted values and CIs to probabilities. 
m2b.ND$Pass.Rate <- invlogit(m2b.ND$fit)
m2b.ND$Pass.LL   <- invlogit(m2b.ND$fit.LL)
m2b.ND$Pass.UL   <- invlogit(m2b.ND$fit.UL)

# Compute unconditional pass rates.
m2b.ND$Pass.URate <- c(cumprod(m2b.ND[m2b.ND$COPIC == -4, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == -3, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == -2, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == -1, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == 0, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == 1, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == 2, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == 3, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == 4, "Pass.Rate"]))

ShowVars <- c("Level", "COPIC", "OPIC", "Pass.Rate", "Pass.LL", "Pass.UL", "Pass.URate")
T29 <- m2b.ND[, ShowVars] 
kable(T29, format = "latex", booktabs = TRUE, digits = 2, 
      format.args = list(nsmall = 2),
      caption = paste("Conditional Pass Rates (with 95 percent CIs) and",
                      "Unconditional Pass Rates by Level Transition and Course")) %>% 
kable_styling(latex_options = c("repeat_header")) 
```

\FloatBarrier

## Odds-Ratios for COPIC Effect    
We exponentiate combinations of the parameters to obtain the odds-ratios showing
the effect of COPIC at each level. This quantifies the effect of COPIC, which
this model assumes varies across level transitions. We are using contrasts that
estimate the simple slope of COPIC at each level, then get simultaneous 95% CIs
that are adjusted for multiple testing via Westfall's (1997) method.

```{r}
# Create objects to hold row names and a contrast matrix (K).
RN <- c("Slope @ L1", "Slope @ L2", "Slope @ L3", "Slope @ L4",
        "Slope diff: L1 - L2", "Slope diff: L1 - L3",
        "Slope diff: L1 - L4", "Slope diff: L2 - L3",
        "Slope diff: L2 - L4", "Slope diff: L3 - L4")
K <- matrix(c(0, 0, 0, 0, 1, 0, 0, 0, 
              0, 0, 0, 0, 1, 1, 0, 0, 
              0, 0, 0, 0, 1, 0, 1, 0, 
              0, 0, 0, 0, 1, 0, 0, 1, 
              0, 0, 0, 0, 0, 1, 0, 0, 
              0, 0, 0, 0, 0, 0, 1, 0, 
              0, 0, 0, 0, 0, 0, 0, 1, 
              0, 0, 0, 0, 0, 1, -1, 0, 
              0, 0, 0, 0, 0, 1, 0, -1, 
              0, 0, 0, 0, 0, 0, 1, -1),
            nrow=10, byrow = TRUE,
            dimnames = list(RN, names(coef(m2b))))

# Run multiple comparisons examine the course effect & get adjusted 95% CIs. 
m2b.ct <- glht(m2b, linfct = K)
m2b.mc <- summary(m2b.ct, test = adjusted("Westfall"))
m2b.ci <- confint(m2b.ct, calpha = adjusted_calpha(test = "Westfall"))
T30 <- data.frame(Est   = m2b.mc$test$coefficients, 
                  SE    = m2b.mc$test$sigma, 
                  CI.LL = m2b.ci$confint[, "lwr"],
                  CI.UL = m2b.ci$confint[, "upr"],
                  OR    = exp(m2b.mc$test$coefficients),
                  OR.LL = exp(m2b.ci$confint[, "lwr"]),
                  OR.UL = exp(m2b.ci$confint[, "upr"]),
                  z     = m2b.mc$test$tstat,
                  p     = m2b.mc$test$pvalues,
                  Sval  = p2s(m2b.mc$test$pvalues),
                  BFB   = p2bfb(m2b.mc$test$pvalues),
                  PPH1  = p2pp(m2b.mc$test$pvalues))
T30$OR[5:10]    <- NA
T30$OR.LL[5:10] <- NA
T30$OR.UL[5:10] <- NA
kable(T30, format = "latex", booktabs = TRUE, format.args = list(digits = 3),
      digits = c(rep(x = 2, times = 8), Inf, 2, 2, 2),
      caption = paste("Simple Slopes of COPIC Effect, Westfall (1997)",
                      "Adjustment for Multiplicity")) %>% 
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

\FloatBarrier

## Assessing Assessing Goodness of Fit, Discrimination, and Calibration
Since we are running the continuation-ration model as a logistic regression, all
the stadard methods for assessing goodness of fit for logistic regresion models
apply.

\FloatBarrier

### Hosmer-Lemeshow goodness of fit test. 
The null hypothesis for the Hosmer-Lemeshow test is that the data fit the model,
so a significant result is actually undesirable because it says that the data
don't fit the model. This is a measure of calibration (Fenlon et al., 2018), 
which can also be examined with a calibration plot like the one shown below.

The calibration plot uses the same set of bins used for the HLT. We used a
binning method that creates bins by grouping the predicted probabilities into
bins with a minimum width of 0.17 because that way all the values grouped
together are quite similar. We tried a bin width of 0.10 as with the other 
models, Unlikebut that yielded bins with fewwer than 15 observations, for which
comparisons may not be meaningful. We aimed for the narrowest bins that had 
adequate size by adding 0.01 to the minimum bin width at each step until all 
bins had $N \ge 15$ observations.

```{r}
HLT.m2b <- HLfit(m2b, bin.method = "prob.bins", min.prob.interval = 0.17, 
                 main = "Calibration Plot", 
                 xlab = "Predicted Probability (minimum bin interval = 0.17)",
                 ylab = "Observed Probability")
HLT.m2b
```


```{r}
# Get the s-value, BFB, and posterior probability of H1. 
convertp(p = HLT.m2b$p.value, digits = 2)
```

\FloatBarrier

### Brier Score
The unscaled Brier score is an overall measure of accuracy suitable for logistic
regression models with binary outcomes (Fenlon et al., 2018). It is the average
prediction error (Steyerberg et al., 2001; Steyerberg et al., 2010). The scaled 
Brier score adjusts for unscaled version to have a range of from 0 to 1, thereby
making it similar to an $R^{2}$ statistic. We report the scaled Brier score 
below.  

```{r}
brier(m2b, digits = 2)
```

\FloatBarrier

### Classification 
Here we want to examine the classification table, plus the overall accuracy,
sensitivity, and specificity estimates.

```{r}
df_crm$pred.m2b <- predict(m2b, type = "response")
set.seed(3179) # For reproducible bootstrap estimates.
roc.m2b <- roc(pass ~ pred.m2b, data = df_crm, ci = TRUE, direction = "<",
               ci.method = "bootstrap", boot.n = 10000)
round(lrcm(roc.m2b, seed = 7146), digits = 3)
```

\FloatBarrier

### Area Under the Curve (AUC)
The area under the receiver operating characteristic curve (AUC) measures the
model's ability to discriminate those who pass from those who fail to pass the
level transitions. It can range from 0.50 to 1.00; values of 0.50--0.69 are
poor, 0.70-0.79 are acceptable, .80-.89 are excellent, and $\geq$ 0.90 are
outstanding.

```{r}
print(roc.m2b)
plot.roc(roc.m2b, print.auc = TRUE, main = "ROC Curve, Model 2b", 
         print.thres = "best")
```

Next we compare the ROC curves for models m2a and m2b using a bootstrap test for
the difference between the AUC values (Robin et al., 2011).

```{r}
set.seed(1128) # For reproducible bootstrap estimates.
roct.m2a.m2b <- roc.test(roc.m2a, roc.m2b, method = "bootstrap", boot.n = 10000)
roct.m2a.m2b
```

```{r}
# Get the s-value, BFB, and posterior probability of H1. 
convertp(p = roct.m2a.m2b$p.value, digits = 2)
```

\FloatBarrier

### $R^{2}$ Measures
Pseudo-$R^{2}$ ($R^{2}_p$) is the squared Pearson correlation between observed
and predicted values, as suggested in Hosmer, Lemeshow, & Sturdivant (2013, p.
182). Meanwhile, $R^{2}_{Dev}$ is a measure based on deviance residuals (Fox,
1997, p. 451; Cameron & Windmeijer, 1997).

```{r}
c(PseudoR2 = pseudoR2(m2b, digits = 2), R2Dev = R2Dev(m2b, digits = 2))
```

\FloatBarrier

## Diagnostics
We need to run model diagnostics to see if there are any obvious problems with
the model. First we check for outliers and find that there are none of note.

```{r}
outlierTest(m2b)
```

Now we look at some residual plots.
```{r}
residualPlots(m2b, layout = c(1,3))
```

Now we look at some index plots for influence measures. We are looking for
observations with values exceeding the cutoffs.

```{r}
# Plot Cook's Distance vs observation index.
PlotCookD(m2b)

# How many cases have high Cook's Distances (D > cutoff)?
summary(cooks.distance(m2b))
addmargins(table(cooks.distance(m2b) > CookDco(m2b)))

# Plot Leverage (hat) values vs. observation index.
PlotHat(m2b)

# Plot Standardized Pearson Residuals vs Leverage, w/ Cook's D contours.
plot(m2b, which = 5, id.n = 10, cex.id = .6, 
     cook.levels = round(CookDco(m2b), digits = 3))
abline(v = hatco(m2b), lty = 2, col = "blue")
text(x = hatco(m2b), y = 5.7, pos = 4, col = "blue", cex = .75,
     labels = paste("Leverage cutoff >", round(hatco(m2b), digits = 3)))

# How many cases have high leverage (hat > cutoff)?
summary(hatvalues(m2b))
addmargins(table(hatvalues(m2b) > hatco(m2b)))

# Identify cases w/ high leverage and high Cook's D. 
InfCases(m2b)
```

\FloatBarrier

## Graphs

\FloatBarrier

### Interaction Effect

We visualize below the level-transition-specific OPIc effect. The effect of OPIc
decreases as we go from the lowest to the highest level transition, as evidenced
by the decreasingly steep slope from the left-most to the right-most panel.

```{r}
visreg(m2b, xvar = "COPIC", by = "Level", layout = c(4, 1), jitter = TRUE,
       strip.names = TRUE, scale = "linear", 
       xlab = "Centered OPIC Score (OPIC - 5)", 
       ylab = "Log odds (Pass)")
visreg(m2b, xvar = "COPIC", by = "Level", layout = c(4, 1), jitter = TRUE,
       strip.names = TRUE, scale = "response", rug = 2,
       xlab = "Centered OPIC Score (OPIC - 5)", 
       ylab = "Conditional Pass Rate")
```

\FloatBarrier

### Conditional Pass Rates
Use ggplot to generate the graph for conditional pass rates derived from model
m2b.

```{r}
F3 <- ggplot(T29, aes(Level, Pass.Rate, group=OPIC, color=OPIC))+
      geom_line()+ coord_cartesian(ylim = c(0, 1))+
      xlab("SA Level Transition")+
      ylab("Conditional Pass Rate")+
      ggtitle(label="Conditional Pass Rates by OPIC Score and Level Transition")
direct.label(F3, "first.polygons")
```

\FloatBarrier

### Unconditional Pass Rates  
Use ggplot to generate the graph for unconditional pass rates derived from model
m2b.

```{r}
F4 <- ggplot(T29, aes(Level, Pass.URate, group=OPIC, color=OPIC))+
      geom_line()+ coord_cartesian(ylim = c(0, 1))+
      xlab("SA Level Transition")+
      ylab("Unconditional Pass Rate")+
      ggtitle(label="Unconditional Pass Rates by OPIC Score and SA Level Transition")
direct.label(F4, "first.polygons")
```

\FloatBarrier

## Conclusion
Model 2b is better than model 2a because the Level x COPIC interaction is
significant. It has better accuracy and sensitivity than model 1a, but we need
to run another model to decide whether adding a course effect to model 2b will
improve the model. That will be done in model 3a.

\FloatBarrier

# MODEL 3A: PARALLEL COURSE + NON-PARALLEL OPIC EFFECTS

We finally test a model with a parallel course effect and a non-parallel OPIc
effect. We add the course effect into the model after the interaction so that a
sequential test will examine whether it adds any value beyond the non-parallel
OPIC effect.

```{r}
T31 <- summary(m3a)
set.caption("Model 3a Summary")
pander(T31, digits = c(5, 5, 4, 3), round = c(4, 4, 2, Inf), justify = "right",
       caption = "Model 3a Summary")
```

Post-process the raw model output to get additional information, including 
s-values, BFBs, and posterior probabilities of H1.

```{r}
format(convertp(T31$coefficients[,"Pr(>|z|)"], digits = 2), digits = 3)
```

\FloatBarrier

## Sequential Tests (Type I SS)
Each row in Table 33 tests the significance of unique additional variance
explained by the term on that line after controlling for all previously entered
terms. Significant results mean adding that term improved the model. Note that 
course actually entered the model last. 

```{r}
T33 <- anova(m3a, test = "Chisq")
T33 <- cbind(T33, convertp(T33[,"Pr(>Chi)"]))
pander(T33, justify = "right", split.tables = Inf, 
       digits = c(5, 5, 5, 8, 3, 3, 4, 3, 3), 
       round = c(0, 3, 0, 3, Inf, 2, 2, 2), 
       caption = "Model 3a Sequential Tests (Type I SS): Analysis of Deviance")
```

\FloatBarrier

## Simultaneous Tests of Main/Interaction Effects via LRT (Type III SS)
The simultaneous tests in Table 34 are the effects of the indicated terms after
controlling for all other terms in the model. They are only computed for terms
that are not part of a higher-order interaction because it makes no sense to
test for a main effect when the variable is involved in an interaction. These
should be functionally equivalent to the results you get with *anova()* when you
feed it a pair of nested models that differ only in that one model includes a
term that is absent from the other model. They are likelihood ratio tests
(LRTs).

```{r}
T34 <- drop1(m3a, test = "Chisq")   
T34 <- cbind(T34, convertp(T34[,"Pr(>Chi)"]))
pander(T34, justify = "right", split.tables = Inf, 
       digits = c(5, 8, 5, 8, 3, 4, 3, 3), 
       round = c(0, 3, 0, 3, Inf, 2, 2, 2),
       caption = "Model 3a Simultaneous Tests (Type III SS)")
```

The fact that the course effect is not significant in Table 34 tells us that we
can really revert back to model 2b because course doesn't have an effect after
controlling for level, COPIC, and their interaction.

\FloatBarrier

## Conditional and Unconditional Pass Rates
We omit calculating these rates because model 3a is not better than model 2b.

\FloatBarrier

## Odds-Ratio for COPIC Effect 
We omit calculating odds-ratios because model 3a is not better than model 2b.

\FloatBarrier

## Assessing Goodness of Fit, Discrimination, and Calibration
Since we are running the continuation-ration model as a logistic regression, all
the stadard methods for assessing goodness of fit for logistic regresion models
apply.

\FloatBarrier

### Hosmer-Lemeshow goodness of fit test. 
The null hypothesis for the Hosmer-Lemeshow test is that the data fit the model,
so a significant result is actually undesirable because it says that the data
don't fit the model. This is a measure of calibration (Fenlon et al., 2018), 
which can also be examined with a calibration plot like the one shown below.

The calibration plot uses the same set of bins used for the HLT. We used a
binning method that creates bins by grouping the predicted probabilities into
bins with a minimum width of 0.10 because that way all the values grouped
together are quite similar.

```{r}
HLT.m3a <- HLfit(m3a, bin.method = "prob.bins", min.prob.interval = 0.1, 
                 main = "Calibration Plot", 
                 xlab = "Predicted Probability (minimum bin interval = 0.10)",
                 ylab = "Observed Probability")
HLT.m3a
```

```{r}
# Get the s-value, BFB, and posterior probability of H1. 
convertp(p = HLT.m3a$p.value, digits = 2)
```

\FloatBarrier

### Classification 
Here we want to examine the classification table, plus the overall accuracy,
sensitivity, and specificity estimates.

```{r}
df_crm$pred.m3a <- predict(m3a, type = "response")
set.seed(9173) # For reproducibility of bootstrap estimates.
roc.m3a <- roc(pass ~ pred.m3a, data = df_crm, ci = TRUE, direction = "<",
               ci.method = "bootstrap", boot.n = 10000)
round(lrcm(roc.m3a, seed = 7146), digits = 3) 
```

\FloatBarrier

### Brier Score
The unscaled Brier score is an overall measure of accuracy suitable for logistic
regression models with binary outcomes (Fenlon et al., 2018). It is the average
prediction error (Steyerberg et al., 2001; Steyerberg et al., 2010). The scaled 
Brier score adjusts for unscaled version to have a range of from 0 to 1, thereby
making it similar to an $R^{2}$ statistic. We report the scaled Brier score 
below.  

```{r}
brier(m3a, digits = 2)
```

\FloatBarrier

### Area Under the Curve (AUC)
The area under the receiver operating characteristic curve (AUC) measures the
model's ability to discriminate those who pass from those who fail to pass the
level transitions. It can range from 0.50 to 1.00; values of 0.50--0.69 are
poor, 0.70-0.79 are acceptable, .80-.89 are excellent, and $\geq$ 0.90 are
outstanding.

```{r}
print(roc.m3a)
plot.roc(roc.m3a, print.auc = TRUE, main = "ROC Curve, Model 3a", 
         print.thres = "best")
```

Next we compare the ROC curves for models m2a and m2b using a bootstrap test for
the difference between the AUC values (Robin et al., 2011). 

```{r}
set.seed(1128) # For reproducible bootstrap estimates.
roct.m2a.m3a <- roc.test(roc.m2a, roc.m3a, method = "bootstrap", boot.n = 10000)
roct.m2a.m3a
```

```{r}
# Get the s-value, BFB, and posterior probability of H1. 
convertp(p = roct.m2a.m3a$p.value, digits = 2)
```

\FloatBarrier

### $R^{2}$ Measures
Pseudo-$R^{2}$ ($R^{2}_p$) is the squared Pearson correlation between observed
and predicted values, as suggested in Hosmer, Lemeshow, & Sturdivant (2013, p.
182). Meanwhile, $R^{2}_{Dev}$ is a measure based on deviance residuals (Fox,
1997, p. 451; Cameron & Windmeijer, 1997).

```{r}
c(PsedoR2 = pseudoR2(m3a, digits = 2), R2Dev = R2Dev(m3a, digits = 2))
```

\FloatBarrier

## Diagnostics
We omit diagnostics because model 3a is not better than model 2b.

\FloatBarrier

## Graphs
We omit graphs because model 3a is not better than model 2b. 

\FloatBarrier

## Conclusion
What distinguished this model from model 2b was inclusion of the parallel course
effect, but the Type III LRT for that effect is not-significant. We will report
model 2b over this model on the basis of parsimony.

\FloatBarrier

# WRAP UP

\FloatBarrier

## Project Information
* CSTAT Client: Paula Winke
* CSTAT CaseID: 18-009

\FloatBarrier

## Funding Sources
Winke, P., Gass, S., & Pierce, S. J. (08/01/2017-07/31/2018). Michigan State
   University Language Flagship Proficiency Initiative Year 4 continuation 
   proposal. [Award #: 0054-MSU-22-PI-280-PO2, $249,865]. Sponsor: Institute
   of International Education. Location: Michigan State University, East 
   Lansing, MI. 

Winke, P., Gass, S., & Pierce, S. J. (01/01/2019–12/31/2019). Michigan State
   University Language Flagship Proficiency Initiative Year 5 continuation
   proposal. [Award #: 0054-MSU-22-PI-280-PO2, Awarded, $37,634]. Sponsor:
   Institute of International Education. Location: Michigan State University, 
   East Lansing, MI. 

\FloatBarrier

## Document Information
Folder: *`r getwd()`*

* Script: *`r knitr:::current_input()`*
* Output: *`r sub(".Rmd", ".pdf", knitr:::current_input())`*

\FloatBarrier

## References
Benjamin, D. J., & Berger, J. O. (2019). Three recommendations for improving 
the use of p-values. *The American Statistician, 73*(Supplement 1), 186-191. 
[doi:10.1080/00031305.2018.1543135](https://doi.org/10.1080/00031305.2018.1543135)

Cameron, A. C., & Windmeijer, F. A. G. (1997). An R-squared measure of goodness
of fit for some common nonlinear regression models. *Journal of Econometrics,
77*(2), 329-342. 
[doi:10.1016/S0304-4076(96)01818-0](https://doi.org/10.1016/S0304-4076(96)01818-0)

Colquhoun, D. (2019). The false positive risk: A proposal concerning what to 
do about p-values. *The American Statistician, 73*(Supplement 1), 192-201. 
[doi:10.1080/00031305.2018.1529622](https://doi-org/10.1080/00031305.2018.1529622)

Fenlon, C., O’Grady, L., Doherty, M. L., & Dunnion, J. (2018). A discussion of 
calibration techniques for evaluating binary and categorical predictive models.
*Preventive Veterinary Medicine, 149*, 107-114. 
[doi:10.1016/j.prevetmed.2017.11.018](https://doi.org/10.1016/j.prevetmed.2017.11.018)

Fox, J. (1997). *Applied regression analysis, linear models, and related
methods*. Thousand Oaks, CA: Sage Publications.

Greenland, S. (2019). Valid p-values behave exactly as they should: Some 
misleading criticisms of p-values and their resolution with s-values. 
*The American Statistician, 73*(Supplement 1), 106-114. 
[doi:10.1080/00031305.2018.1529625](https://doi.org/10.1080/00031305.2018.1529625)

Hosmer, D. W., Lemeshow, S., & Sturdivant, R. X. (2013). *Applied logistic
regression* (3rd ed.). Hoboken, NJ: John Wiley & Sons, Inc.

Perkins, N. J., & Schisterman, E. F. (2006). The inconsistency of “optimal”
cutpoints obtained using two criteria based on the receiver operating
characteristic curve. *American Journal of Epidemiology, 163*(7), 670-675.
[doi:10.1093/aje/kwj063](https://doi-org/10.1093/aje/kwj063)

Raykov, T., & Marcoulides, G. A. (2011). *Introduction to psychometric theory*.
New York, NY: Routledge.

Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., & 
Müller, M. (2011). pROC: an open-source package for R and S+ to analyze and 
compare ROC curves. *BMC Bioinformatics, 12*, 77. doi:10.1186/1471-2105-12-77

Steyerberg, E. W., Harrell Jr., F. E., Borsboom, G. J. J. M., Eijkemans, M. J. 
C., Vergouwe, Y., & Habbema, J. D. F. (2001). Internal validation of predictive 
models: Efficiency of some procedures for logistic regression analysis. Journal 
of Clinical Epidemiology, 54(8), 774-781. 
[doi:10.1016/S0895-4356(01)00341-9](https://doi.org/10.1016/S0895-4356(01)00341-9)

Steyerberg, E. W., Vickers, A. J., Cook, N. R., Gerds, T., Gonen, M., 
Obuchowski, N. A., . . . Kattan, M. W. (2010). Assessing the performance of 
prediction models : A framework for traditional and novel measures. 
Epidemiology, 21(1), 128-138. 
[doi:10.1097/EDE.0b013e3181c30fb2](http://doi.org/10.1097/EDE.0b013e3181c30fb2)

Wasserstein, R. L., Schirm, A. L., & Lazar, N. A. (2019). Moving to a world 
beyond "p < .05". *The American Statistician, 73*(Supplement 1), 1-19. 
[doi:10.1080/00031305.2019.1583913](https://doi.org/10.1080/00031305.2019.1583913)

Westfall, P. H. (1997). Multiple testing of general contrasts using logical 
constraints and correlations. *Journal of the American Statistical Association, 
92*(437), 299-306. 
[doi:10.1080/01621459.1997.10473627](https://doi.org/10.1080/01621459.1997.10473627)

Winke, P., Zhang, X., & Pierce, S. J. (2019). *Self-assessment works! 
Continuation-ratio models for testing course and OPIc score effects on oral
proficiency self-assessments*. Michigan State University, East Lansing, MI.
Manuscript in preparation.

Winke, P., Pierce, S. J., & Zhang, X. (2018, October). Self-assessment works! 
Continuation-ratio models for testing course and OPIc score effects on oral 
proficiency self-assessments. Paper presented at the East Coast Organization 
of Language Testers 2018 conference, hosted by the Educational Testing Service,
Princeton, NJ. https://sites.google.com/site/ecoltaelrc/home

Youden, W. J. (1950). Index for rating diagnostic tests. *Cancer, 3*(1), 32-35.
[doi:10.1002/1097-0142(1950)3%3A1%3C32%3A%3AAID-CNCR2820030106%3E3.0.CO%3B2-3](http://dx.doi.org/10.1002/1097-0142(1950)3%3A1%3C32%3A%3AAID-CNCR2820030106%3E3.0.CO%3B2-3)

\FloatBarrier

## Software Information
We use R Markdown to enhance reproducibility. Knitting the R Markdown script
generates the PDF file containing explanatory text, R code, plus R output (text
and graphics) noted in the Document Information section above.
  
* We use [RStudio (version 1.2.5019 or later)](www.rstudio.org) to work with 
  R and R markdown files. The software chain looks like this:
  **Rmd file -> RStudio -> R -> knitr -> pandoc -> TinyTeX -> PDF file**.
* We recommend using [TinyTeX](https://yihui.org/tinytex/) to compile LaTeX files 
  into PDF files. However, it should be viable to use 
  [MiKTeX version 2.9](https://miktex.org) instead. 
  
\FloatBarrier

## Show Software Citations & Version Information.
``` {r show_citations}
sessionInfo()
citation("polycor")
citation("car")
citation("gtools")
citation("multcomp")
citation("visreg")
citation("knitr")
citation("texreg")
citation("pander")
citation("pROC")
citation("ggplot2")
citation("tidyr")
citation("directlabels")
citation("lattice")
citation("modEvA")
citation("piercer")
```
