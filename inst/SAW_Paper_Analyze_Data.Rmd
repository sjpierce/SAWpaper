---
title: \sffamily{\bfseries{\LARGE Self-Assessment Works! Paper Analyze Data}}
geometry: "left=1.0in,right=1.0in,top=.75in,bottom=.75in"
author: "Steven J. Pierce & Xiaowan Zhang"
output:
 pdf_document:
   latex_engine: xelatex
   number_sections: true
   toc: yes
   toc_depth: 3
urlcolor: blue
header-includes:
- \usepackage{fancyhdr}
- \usepackage[yyyymmdd,hhmmss]{datetime}
- \usepackage{lastpage}
- \usepackage{amsmath,amsthm}
- \usepackage{unicode-math}
- \defaultfontfeatures{Ligatures=TeX}
- \usepackage[font={small}, margin=1cm, skip=2pt]{caption}
- \usepackage{url}
- \usepackage{floatrow} 
- \floatplacement{figure}{!ht}
- \floatplacement{table}{!ht}
- \usepackage{placeins}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{dcolumn}
- \usepackage{titling}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage{lscape}
- \pagestyle{fancy}
- \lhead{Self-Assessment Works! Analyze Data}
- \rhead{\today\ \currenttime}
- \cfoot{ }
- \lfoot{\texttt{\footnotesize \detokenize{`r sub(".Rmd", ".pdf", knitr:::current_input(dir = FALSE))`}}} 
- \fancyfoot[R]{\thepage}
- \renewcommand{\headrulewidth}{0.4pt}
- \renewcommand{\footrulewidth}{0.4pt}
- \fancypagestyle{plain}{\pagestyle{fancy}}
---

\FloatBarrier

# Purpose
This file reproduces the results reported in our manuscript (Winke, Zhang, &
Pierce, 2022), which was based on a presentation (Winke, Pierce, & Zhang 2018).
It analyzes data on Spanish language learners who took a Can-Do self-assessment
test, along with the more authoritative OPIc language proficiency test (Winke &
Zhang, 2022). We did both correlation analyses and continuation-ratio models
that examine the effect of course and OPIc speaking proficiency scores on the
passing rate for each level of the Can-Do statements self-assessment. The
objective was to validate the Can-Do test results.

\FloatBarrier

## Target Journal
We submitted this as a "Research Report" to a journal called 
Studies in Second Language Acquisition (SSLA), 
https://www.cambridge.org/core/journals/studies-in-second-language-acquisition.
The author instructions are at
https://www.cambridge.org/core/journals/studies-in-second-language-acquisition/information/instructions-contributors.

We will consider applying for the SSLA *Open Data Badge* and the 
*Open Materials Badge* described at 
https://www.cambridge.org/core/journals/studies-in-second-language-acquisition/open-science-badges
. Hosting a public Github repository should meet the requirements for 
both badges because GitHub is on the 
[Registry of Research Data Repositories](https://www.re3data.org). However, we
may put the actual data in another public repository and just use GitHub for the
code. 

\FloatBarrier

## Comment on Statistical Methodology
We experimented with applying ideas discussed in Wasserstein, Schirm, and Lazar
(2019) about abandoning declarations of statistical significance. This landmark
editorial paper produced by the American Statistical Association discussed
various ways to supplement p-values with additional statistics or replace them 
altogether. We computed some supplemental statistics and include them in this 
set of materials but omitted them from the manuscript. 

We computed Shannon information values (s-values; Greenland, 2019; Wasserstein,
Schirm, & Lazar, 2019). S-values are a rescaling of p-values, such that 
$s = -log_2(p)$. They can range from $s = 0$ when $p = 1$ to $s = \infty$ when 
$p = 0$. Larger s-values correspond to greater evidence against the null 
hypothesis. An s-value can be interpreted as how many bits of information there
are against the hypothesis. To make that easier to understand, suppose you want
to flip a coin repeatedly to determine whether it is a fair coin rather than one
biased toward landing on heads. Each flip provides one bit of information, but
only those resulting in heads are information against the null hypothesis. A
fair coin should yield heads half of the time and tails the other half
(independent with probability = .50 for each outcome on each flip). The null
hypothesis is that the coin is fair. Seeing 2 consecutive heads come up in a set
of 2 flips ($p = 0.5^{2} = 0.25, s = 2$) would be weak evidence against fairness 
but an s-value of 10 would be more persuasive, like getting 10 heads from a set 
of 10 coin flips ($p = 0.5^{10} = 0.0009765625, s = 10$).

Other ideas that look viable to use are reporting minimum false positive risk
(mFPR; Colquhoun, 2009), Bayes Factor Bounds (BFB; Benjamin & Berger, 2019), and 
the upper bound for the posterior probability that that alternate hypothesis
(H1) is true (Benjamin & Berger, 2019). We have opted to use the latter two 
instead of mFPR. 

Taking the Can-Do self-assessment yields an ordinal score ranging from 1-5 that
represents the proficiency level of the learner who took the test. Higher levels
indicate greater proficiency. We can conceptualize the process that yields that
score as a sequential selection process comprised of a set of four level
transition testlets (LTTs) that must be passed in strict order. Every learner
starts at level 1 and only advances to level 2 by passing the first LTT. The
learner's proficiency level is incremented for each LTT passed. The
self-assessment ends as soon as the learner fails to pass an LTT, or reaches the
maximum proficiency level (level 5). For a sample of $N$ learners, there will be
between $N$ and $4N$ binary LTT results (0 = fail, 1 = pass) depending on how 
many learners passed each LTT. 

We use continuation-ratio models to examine the proficiency levels achieved by 
the learners. These are simply logistic regression models applied to a 
reorganized dataset with one row of data per LTT attempted by each learner. 
We are using a predictive modeling approach here that tests how well OPIc scores
and course predict self-assessed proficiency levels. Therefore, we evaluate the
models according to both discrimination and calibration criteria relevant to 
predictive models (Fenlon, O'Grady, Doherty, & Dunnion, 2018). 

\FloatBarrier

# Setup
Set global R chunk options (local chunk options will over-ride global options). 

``` {r global-options, cfsize = "footnotesize"}
# Create a custom chunk hook/option for controlling font size in chunk & output.
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$cfsize != "normalsize", paste0("\n \\", options$cfsize,"\n\n", 
                                              x, "\n\n \\normalsize"), x)
  })

# Global chunk options (over-ridden by local chunk options)
knitr::opts_chunk$set(include  = TRUE, echo = TRUE, error = TRUE, 
                      message = TRUE, warning = TRUE, fig.pos = "!ht", 
                      cfsize = "footnotesize")

# Declare location of this script relative to the project root directory.
here::i_am(path = "inst/SAW_Paper_Analyze_Data.Rmd")
```

\FloatBarrier

## Load Packages and Set Package Options
Load contributed R packages that we need to get additional functions. 

``` {r load-packages}
library(here)              # for here()
library(polycor)           # for hetcor()
library(car)               # for residualPlots(), influenceIndexPlot(), outlierTest()
library(multcomp)          # for glht()
library(visreg)            # for visreg()
# Set package options. 
# options(knitr.kable.NA = '0.00')
library(rmarkdown)         # for render(), pandoc_version(). 
library(knitr)             # for kable()
library(texreg)            # for texreg()
library(pander)            # for pander()
library(pROC)              # for roc()
library(ggplot2)           # for ggplot()
library(tidyr)             # for unite()
library(dplyr)             # for filter(), select(), etc.
library(directlabels)      # for direct.label()
library(lattice)           # for strip.custom()
library(modEvA)            # for HLfit()
# Set package options. 
options(kableExtra.latex.load_packages = FALSE)
library(kableExtra)        # for kable_styling(), add_header_above(), 
                           # column_spec(), collapse_rows(), and landscape()
library(Hmisc)             # for rcorr()
library(piercer)           # for p2s(), p2bfb(), p2pp(), convertp(), lrcm(), 
                           # brier(), r.p(), r.pc(), r.pc(), ci.rpc(), ci.rp(),
                           # git_report(), which_latex(), extract_glm()
library(stringr)           # for word()
library(broom)             # for tidy()
library(SAWpaper)          # for package version number via session_info()
```

\FloatBarrier

## Load Data

The file `SAW_Paper_Data.RData` contains four data frames: `SData`, `TAData`,
`VSData`, and `VTAData`. Table \ref{tab:data-frame-sizes} shows the number of
observations (rows) and variables in each of these datasets.

```{r load-data}
load(file = here::here("data/SAW_Paper_Data.RData"))
```

```{r data-frame-sizes}
data.frame(Dataset = c("SData", "VSData", "TAData", "VTAData"),
           N.Rows = c(nrow(SData), nrow(VSData), nrow(TAData), nrow(VTAData)),
           N.Vars = c(ncol(SData), ncol(VSData), ncol(TAData), ncol(VTAData))) %>% 
  kable(format = "latex", booktabs = TRUE, digits = 0,
        col.names = c("Data Frame", "Rows", "Variables"),
        caption = "Data Frame Sizes") %>%
  add_header_above(header = c("", "Number of ..." = 2)) %>% 
  group_rows(group_label = "Student data", start_row = 1, end_row = 2) %>% 
  group_rows(group_label = "Testlet attempt data", start_row = 3, end_row = 4)
```

The `SData` data frame contains the cleaned student-level data for all Spanish
students in the same, including those with invalid OPIc scores, while the
`VSData` data frame is the subset of data from `SData` containing only data from
students who had valid OPIc score. We use the latter for our correlation
analyses. Meanwhile, the `TAData` data frame contains data about the same set of
students shown in `SData` but has been expanded to one row per student per
self-assessment testlet attempted. The `VTAData` data frame is the subset of
`TAData` containing only data for the students included in `VSDdata`. Therefore,
we use `VTAData` in our continuation-ratio models.

\FloatBarrier

## Data Structure Comments
In `SData` (and thus also in `VSData`), `Level` (and `LevelF`, which is just a
copy of Level that is stored as a factor instead of a numeric variable) denote
the five levels of Can-Do statements on the self-assessment, whereas in `TAData`
(and thus also in `VTAData`) the `Testlet` variable represents the transition
testlets that control passage from one level to the next. The relationship
between these variables is illustrated in this diagram:
$1 \rightarrow 2 \rightarrow 3 \rightarrow 4 \rightarrow 5$. `Level` in `SData`
is represented by the numbers, and `Testlet` in `TAData` is represented by the
arrows, which can be numbered 1-4 sequentially from left to right. 

Unlike `SData` and `VSData` where there is one row per learner with `Level`
indicating the maximum level a learner reached on the self-assessment, `TAData`
and `VTAData` have been expanded such that there is one row per learner per
transition testlet attempted. The binary variable, `Pass`, in `VTAData` indexes
whether a learner passed/failed to pass a given transition testlet. For
instance, a learner who reached Level 3 on the self-assessment would have one
row in `VSData` with Level equal to 3. The same learner would have three rows in
`VTAData`, having a 1 on `Pass` for the rows representing testlets 1 and 2, and
a 0 on `Pass` for the row representing testlet 3, which was highest transition
attempted.

The data in `VTAData` is set up as described above for the application of
continuation-ratio modeling, which models the probabilities of passing the four
transitions (represented by `Testlet` in `VTAData`) as a function of predictors
(which, in the case of our study, are course and OPIc speaking scores). Each of
those transitions will eventually yield an estimate of the conditional pass
rate, which is the probability that an individual who reached the level on the
left end of the arrow succeeds in passing on to the level on the right end.
Continuation-ratio models can be conducted as logistic regressions where the
data frame has one row of data per person for each level transition that the
individual actually attempted.

We treat OPIc speaking test proficiency scores as a continuous covariate that
ranges from Novice-low to Superior. The variable `OPICA` in `VSData` shows the
actual OPIc ratings. Numerically transfomed OPIc scores are captured by `OPICV`
in `VSData` (NL = 1, NM = 2, NH = 3, ..., AM = 8, AH = 9, Superior = 10). We
removed the cases for which the OPIc ratings were not meaningful for this study
(AR, BR, and UR). To make the model coefficients more interpretable, we included
centered OPIc scores instead of the original OPIc results in `VSData` and stored
them as `COPIC`. A score of zero in `COPIC` represents the level of
Intermediate-mid on the OPIc scale.

As readers may have noticed, some students with OPIc scores did not have any
numerically transformed OPIc values (i.e., `OPICV` = NA in `SData`). This is
because these students did not receive a valid score on the test. Specifically,
these students received either above range (AR), below range (BR), or unratable
(UR) on the OPIc. AR was most likely awarded when a student selected an test
form that was well below their oral proficiency level. BR was most likely
awarded when a student selected a test form that was well above their
proficiency level. And UR was given when a student's oral response was not
ratable due to one of a variety of reasons (e.g., no response, technology
failure). For transparency purposes, we include those students with non-scored
OPIc tests in the starting datasets, but we have to exclude them from the main
analyses because their proficiency levels as measured by the OPIc were unknown.
See the output from our `inst/SAW_Paper_Import_Explore_Data.Rmd` script for
descriptive data on the students with invalid OPIc scores.

For analysis purposes, we treat the variables of `Course`, `Level`, and
`Testlet` as factors that are effectively ordinal variables and we treat `OPICV`
as a continuous variable. To make model coefficients more interpretable, we
included the centered form of OPIc scores stored in `COPIC` instead of `OPICV` 
in the continuation-ratio models. A score of zero in `COPIC` represents the 
Intermediate-mid level on the OPIc scale.

\FloatBarrier

# Data Visualization
We visualized the relationship of SA total scores with OPIc scores and course
level in the `VSData` dataset using scatterplots. We put students' OPIc scores
on the x-axis and their total SA scores (ranging from 1 to 200) on the Y-axis.
We assigned different colors to individual data points based on course level. We
also jittered the data points so that overlapping points are nudged apart and
can be seen more clearly. We added fit lines to the scatterplots to examine the
shape of the relationship between SA level and OPI scores. These results are not
discussed in the manuscript but were part of our preliminary examination of the
data.

## Overall Relationship
In Figure \ref{fig:plot-SAScore-OPICV-linear}, we start with a linear regression
line to visually check how well a simple parametric form represents the overall
relationship. Then in Figure \ref{fig:plot-SAScore-OPICV-loess}, we added a loess
curve instead. A loess curve is a smooth fit line based on local regression of a
dependent variable (Y-variable) on an independent variable (X- variable). It
helps one to see non-linearity in the relationship between variables. Here we 
substantial evidence if non-linearity because the loess fit line is quite bowed
rather than mostly straight. 

```{r plot-SAScore-OPICV-linear, message=FALSE, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:plot-SAScore-OPICV-linear}",
              "Self-Assessment Total Score as a function of OPIc Score,", 
              "With Linear Fit Line.")

# Objects to store settings for plots.
opic.breaks <- c(1, 2, 3, 4, 5, 6, 7, 8, 9)
opic.labels <- c("NL\n1", "NM\n2", "NH\n3", "IL\n4", "IM\n5", "IH\n6", 
                 "AL\n7", "AM\n8", "AH\n9")
sata.breaks <- c(40, 80, 120, 160, 200)
sata.labels <- c("40\n(Level 1)", "80 \n(Level 2)", "120\n(Level 3)", 
                 "160\n(Level 4)", "200\n(Level 5)")
  
# Linear regression lines added to jittered and colored-coded plots
ggplot(VSData, aes(OPICV, Item1_50))+
  geom_jitter(aes(color=Course))+
  geom_smooth(method="lm", se=FALSE)+
  xlim(1,9)+
  scale_x_continuous(breaks = opic.breaks, labels = opic.labels)+
  scale_y_continuous(breaks = sata.breaks, labels = sata.labels)+
  xlab("OPIc score")+
  ylab("Self-assessment total score")+
  guides(color = guide_legend(title="Course"))
```

```{r plot-SAScore-OPICV-loess, message=FALSE, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:plot-SAScore-OPICV-loess}",
              "Self-Assessment Total Score as a function of OPIc Score,", 
              "With Loess Fit Line.")

# Smooth loess lines added to jittered and colored-coded scatterplots
ggplot(VSData, aes(OPICV, Item1_50))+
  geom_jitter(aes(color=Course))+
  geom_smooth(method="loess", se=FALSE, span = .77)+
  xlim(1,9)+
  scale_x_continuous(breaks = opic.breaks, labels = opic.labels)+
  scale_y_continuous(breaks = sata.breaks, labels = sata.labels)+
  xlab("OPIc score")+
  ylab("Self-assessment total score")+
  guides(color = guide_legend(title="Course"))
```

\FloatBarrier

## Relationships Stratified by Course
Next, we tried stratifying the relationships by course. Figure
\ref{fig:plot-SAScore-OPICV-by-Course-linear} shows four linear regression
lines: one for each course level. Compare that to Figure
\ref{fig:plot-SAScore-OPICV-by-Course-loess} where we show separate loess curves for 
each of the four course levels instead.  

```{r plot-SAScore-OPICV-by-Course-linear, message=FALSE, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:plot-SAScore-OPICV-by-Course-linear}",
              "Self-Assessment Total Score as a function of OPIc Score and", 
              "Course, With Linear Fit Lines.")

ggplot(VSData, aes(OPICV, Item1_50, color=Course))+
  geom_jitter()+
  geom_smooth(method="lm", se=FALSE)+
  xlim(1,9)+
  scale_x_continuous(breaks = opic.breaks, labels = opic.labels)+
  scale_y_continuous(breaks = sata.breaks, labels = sata.labels)+
  xlab("OPIc score")+
  ylab("Self-assessment total score")+
  guides(color = guide_legend(title="Course"))
```

```{r plot-SAScore-OPICV-by-Course-loess, message=FALSE, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:plot-SAScore-OPICV-by-Course-loess}",
              "Self-Assessment Total Score as a function of OPIc Score and", 
              "Course, With Loess Fit Lines.")

ggplot(VSData, aes(OPICV, Item1_50, color=Course))+
  geom_jitter()+
  geom_smooth(method="loess", se=FALSE, span = .93)+
  xlim(1,9)+
  scale_x_continuous(breaks = opic.breaks, labels = opic.labels)+
  scale_y_continuous(breaks = sata.breaks, labels = sata.labels)+
  xlab("OPIc score")+
  ylab("Self-assessment total score")+
  guides(color = guide_legend(title="Course"))
```

\FloatBarrier

# Correlations
We examine the correlations among course level, OPIc scores, and SA level using
the student-level dataset (`VSData`). As we mentioned earlier, `Level`
represents the highest level that a student reached on the SA. Polyserial
correlation is calculated for `OPICV` and `Course` and for `OPICV` and `Level`,
whereas polychoric correlation is calculated for the two ordinal variables,
`Course` and `Level`. We included both ordinal and continuous versions of
selected variables to simplify getting both polychoric and polyserial
correlations and (for comparison purposes only) Pearson correlations that are
less appropriate for measuring some relationships. The names of continuous
versions of variables that should be treated as ordinal have ".n" suffixes. We
also estimated selected Spearman correlations for a similar comparative purpose.

```{r hetcor-output, warning = FALSE}
# Create numeric versions of variables to meet hetcor() function requirements.
VSData <- VSData %>% 
  mutate(OPIC.n = as.numeric(OPICV),
         Level.n = as.numeric(Level),
         Course.n = as.numeric(Course))

# cvars = continuous variables
cvars <- c("OPIC.n", "Item1_50", "Level.n", "Course.n")

# ovars = ordinal variables
ovars <- c("LevelF", "Course")

# Use the hetcor (heterogenous correlations) in "polycor" package to get a      
# matrix with Pearson correlations (if both variables are continuous), 
# polyserial correlations (if one is continuous and the other is ordinal), and 
# polychoric correlations (if both are ordinal). Warning messages from hetcor() 
# occur because polyserial correlations between ordinal and continuous versions
# of the same variable are not sensible. It was just faster to estimate them as
# part of a single larger matrix than individually get the subset we want. 
# So, we disabled reporting warnings from this chunk.

HC <- hetcor(as.data.frame(VSData[, c(cvars, ovars)]), ML=TRUE, 
             use="pairwise.complete.obs")
```

Table \ref{tab:show-rps-rpc} shows the polyserial and polychoric correlations,
along with confidence intervals and associated statistics.

```{r show-rps-rpc}
rps.rpc <- rbind(r.ps(HC, cont = cvars, ord = ovars, digits = 2, 
                      pdigits = NULL),
                 r.pc(HC, ord = ovars, digits = 2, pdigits = NULL)) %>% 
  mutate(Variables = word(rownames(.), 2, sep = "\\:"),
         Type = word(rownames(.), 1, sep = "\\:"),
         Pval = format(Pval, digits = 3)) %>% 
  select(Variables, Type, Cor, SE, CI.LL, CI.UL, Z, Pval, Sval, BFB, PPH1)

kable(rps.rpc, format = "latex", booktabs = TRUE, row.names = FALSE,
      digits = c(Inf, Inf, rep(x = 2, times = 5), Inf, c(2, 2, 2)),
      caption = "Polyserial and Polychoric Correlations") %>%
kable_styling(latex_options = c("repeat_header"))
```

Obtain Pearson correlations among two continuous variables, OPIC and Item1_50 
(SA sum scores ranging from 1 to 200), and two ordinal variables, Course and 
Level. Note that it is inappropriate to use Pearson's r for ordinal variables. 
We include them here only for comparison purposes. 

```{r pearson}
rp <- r.p(HC, cont = cvars, digits = 2, pdigits = NULL)
```

Now obtain Spearman correlations for comparison purposes even though they are 
not technically appropriate for our purpose. 

```{r spearman}
rsa <- ci.rp(r = cor(x = VSData$Level.n, y = VSData$Course.n, 
                     method = "spearman"), 
             n = nrow(VSData), rn = "r.s: Level and Course")
rsb <- ci.rp(r = cor(x = VSData$Level.n, y = VSData$OPIC.n, 
                     method = "spearman"), 
             n = nrow(VSData), rn = "r.s: Level and OPIC")
rsc <- ci.rp(r = cor(x = VSData$Course.n, y = VSData$OPIC.n, 
                     method = "spearman"), 
             n = nrow(VSData), rn = "r.s: Course and OPIC")
```

Table \ref{tab:show-rp-rs} shows the Pearson and Spearman correlations,
along with confidence intervals and associated statistics. 

```{r show-rp-rs}
# Bind into a data frame 
rp.rs <- rbind(rp, rsa, rsb, rsc) %>% 
  mutate(Variables = word(rownames(.), 2, sep = "\\:"),
         Type = word(rownames(.), 1, sep = "\\:"),
         Pval = format(Pval, digits = 3),
         BFB = format(BFB, digits = 3))  %>% 
  select(Variables, Type, Cor, SE, CI.LL, CI.UL, t, df, Pval, Sval, BFB, 
         PPH1)

kable(rp.rs, format = "latex", booktabs = TRUE, row.names = FALSE,
      digits = c(rep(x = 2, times = 8), Inf, c(2, 2, 2)),
      caption = "Pearson and Spearman Correlations") %>%
kable_styling(latex_options = c("repeat_header"))
```

Notice that the t and z-statistics are all really large, so the p-values are
zero. That causes all the s-values to become infinite, and the BFB values to
become NaN (not-a-number) because we are dividing by zero in the denominator.
You can read more about interpreting s-values and BFB values in Greenland
(2019), Benjamin & Berger (2019), and Wasserstein, Schirm, & Lazar (2019).

\FloatBarrier

# Fit Continuation-Ratio Models
Here we fit a series of continuation-ratio models to `VTAData` to see whether
course level and OPIc scores significantly predict the conditional pass rate for
each transition on the SA. A parallel effect for a predictor is one where the
predictor's effect is constant across the transition testlets while a
non-parallel effect allows it to vary across testlets. Models 1a and 1b focus
primarily on whether there is a parallel or non-parallel course effect. Models
2a and 2b primarily test whether there is a parallel or non-parallel effect of
OPIc score *and are the models reported in our published manuscript*. Finally,
Models 3a and 3b include both course and OPIc score as predictors.

```{r fit-models}
# Model 1a: Parallel course effect
m1a <- glm(Pass ~ Testlet + Course - 1, data = VTAData, family = "binomial")

# Model 1b: Non-parallel course effect
m1b <- glm(Pass ~ Testlet + Course + Testlet*Course - 1, data = VTAData, 
           family = "binomial")

# Model 2a: Parallel OPIC effect
m2a <- glm(Pass ~ Testlet + COPIC - 1, data = VTAData, family = "binomial")

# Model 2b: Non-parallel OPIC effect
m2b <- glm(Pass ~ Testlet + COPIC + Testlet*COPIC - 1, data = VTAData, 
           family = "binomial")

# Model 3a: Parallel OPIC + parallel course effect
m3a <- glm(Pass ~ Testlet + COPIC + Course - 1, data = VTAData, 
           family = "binomial")

# Model 3b: Non-parallel OPIC + parallel course effect
m3b <- glm(Pass ~ Testlet + COPIC + Testlet:COPIC + Course - 1, data = VTAData, 
           family = "binomial")

# Create a list of the model fit objects.
TModels <- list(m1a, m1b, m2a, m2b, m3a, m3b)

# Create a list of texreg objects for the models.
TModels.TR <- lapply(X = TModels, FUN = extract_glm)
```

Table \ref{tab:show-models} shows the model parameters for the whole set of
models, along with various goodness of fit statistics. The Brier score is an
overall measure of accuracy suitable for logistic regression models with binary
outcomes (Fenlon et al., 2018). It is the average prediction error (Steyerberg
et al., 2001; Steyerberg et al., 2010). The scaled Brier score adjusts the
unscaled version to have a range of from 0 to 1, thereby making it similar to an
$R^{2}$ statistic. Values close to 1 are desirable and indicate good
calibration. We report the scaled Brier score below. Pseudo-$R^{2}$ ($R^{2}_p$)
is the squared Pearson correlation between observed and predicted values, as
suggested in Hosmer, Lemeshow, & Sturdivant (2013, p. 182). Meanwhile,
$R^{2}_{Dev}$ is a measure based on deviance residuals (Fox, 1997, p. 451;
Cameron & Windmeijer, 1997).

\begin{landscape}

```{r show-models, echo = FALSE, results='asis'} 
texreg(TModels.TR, use.packages = FALSE, dcolumn = TRUE, booktabs = TRUE, 
       single.row = TRUE, digits = 2, label = "tab:show-models", 
       custom.model.names = c("Model 1a",  "Model 1b", "Model 2a", "Model 2b",
                              "Model 3a", "Model 3b"), 
       custom.gof.names = c("Null model deviance",
                            "Null model $df$",
                            "Log Likelihood",
                            "$AIC$",
                            "$BIC$",
                            "Deviance", 
                            "Residual $df$",
                            "No. observations", 
                            "Brier score",
                            "Pseudo-$R^{2}$ ($R^{2}_p$)",
                            "$R^{2}_{Dev}$"),
       threeparttable = TRUE,
       custom.note = paste("\\item %stars" , 
                           "\\item Only models 2a and 2b were reported in",
                           "our paper."),
       caption = paste("Continuation-Ratio Model Parameter Estimates,",
       "Standard Errors, and Goodness of Fit Statistics"))
```

\end{landscape}

In subsequent sections, we examine each of the fitted models in detail by
extracting, post-processing, and displaying model parameters, predicted values,
and supplemental statistics, hypothesis tests, and model diagnostics. For
example, we examine sequential tests of model terms based on Type I sums of
squares. Those test the significance of unique additional variance explained by
the term on that line after controlling for all previously entered terms (but 
ignoring terms that enter the model later). Significant results mean adding that
term improved the model.

We also examine simultaneous tests of predictors, which are the effects of the
indicated terms after controlling for all other terms in the model. They are
only computed for terms that are not part of a higher-order interaction because
it makes no sense to test for a main effect when the variable is involved in an
interaction. These should be functionally equivalent to the results you get with
*anova()* when you feed it a pair of nested models that differ only in that one
model includes a term that is absent from the other model. They are likelihood
ratio tests (LRTs).

We used odds-ratios to quantify effect sizes for both specific model parameters
and for contrasts derived from combining model parameters. When using contrasts
to do things like pairwise comparisons, we obtain simultaneous 95% CIs that are
adjusted for multiple testing via Westfall's (1997) method.

We used the inverse logit transformation to convert fitted values and associated
confidence intervals into the conditional probability of passing a particular
level transition given that specific values on the predictors. We computed the
unconditional pass rates from the conditional pass rates as sets of cumulative
products. Tabulating and plotting these conditional and unconditional rates
provides deeper understanding of the model results.

Since we are running the continuation-ration models as logistic regressions, all
the standard methods for assessing goodness of fit for logistic regresion models
apply. We applied the Hosmer-Lemeshow test (HLT), wherein the null hypothesis is
that the data fit the model. Thus, for the HLT a significant result is actually
undesirable because it says that the data don't fit the model. This is a measure
of calibration (Fenlon et al., 2018), which can also be examined with
calibration plots. These plot split the data into the same set of bins used for
the HLT, by grouping the predicted probabilities into bins with a minimum width
(typically 0.10 because that way all the values grouped together are quite
similar). The confidence interval for each bin is labeled with the bin's sample
size. Ideally, the point estimate for each bin is close to the dashed line, but
at a minimum we want all the confidence intervals to overlap that dashed line.

We also use functions from the pROC package to identify the optimal cutpoint $c$
to use when converting fitted probabilities of passing a testlet into a binary
prediction of whether the individual will pass it. The naieve cutpoint would be
$c = 0.5$, but that is not necessarily the optimal cutpoint. Choosing a cutpoint
via use of the Youden index (Youden, 1950) is optimal with respect to overall
misclassification rate for a given weighting of sensitivity and specificity
(Perkins & Schisterman, 2006). Thus, we show the results of various
classification measures given that we use the optimal cutpoint based on the
Youden index. Here are short definitions of what some of these statistics 
measure. 

* Accuracy is the model's overall ability to correctly classify learners
  according to whether they pass versus fail the level transitions. 
* Sensitivity is the ability of the model to correctly identify learners who 
  passed level transitions. 
* Specificity is the ability of the model to correctly identify learners who 
  fail the level transitions. 

Table \ref{tab:classification-measures} shows a list of the classification 
measures computed for each model, along with the label used in tables later to
identify each measure. 

```{r classification-measures}
data.frame(Measure = c("Threshold value", "Specificity", "Sensitivity", 
                       "Accuracy", "True negative count", "True positive count",
                       "False negative count", "False positive count",
                       "Negative predictive value", "Positive predictive value",
                       "False discovery rate", "False postivie rate",
                       "True positive rate", "True negative rate",
                       "False negative rate", "1 - Specificity",
                       "1 - Sensitivity", "1 - Accuracy",
                       "1 - Negative predictive value", 
                       "1 - Positive predictive value", "Precision", "Recall",
                       "Youden Index", 
                       "Distance to Top Left Corner of the ROC space"),
           Label = c("threshold", "specificity", "sensitivity", "accuracy",
                     "tn", "tp", "fn", "fp", "npv", "ppv", "fdr", "fpr", "tpr", 
                     "tnr", "fnr", "1-specificity", "1-sensitivity", 
                     "1-accuracy", "1-npv", "1-ppv", "precision", "recall",
                     "youden", "closest.topleft")) %>% 
  kable(format = "latex", booktabs = TRUE, row.names = FALSE,
        caption = "Classification Measures Reported for Each Model")
```

The area under the receiver operating characteristic curve (AUC) measures the
model's ability to discriminate those who pass from those who fail to pass the
level transitions. It can range from 0.50 to 1.00; values of 0.50--0.69 are
poor, 0.70-0.79 are acceptable, .80-.89 are excellent, and $\geq$ 0.90 are
outstanding. These interpretive heuristics come from Hosmer, Lemeshow, &
Sturdivant (2013, p. 177). Thus, for each model we plot the ROC curve for the
model and annotate it with the best classification threshold, plus the
corresponding values of sensitivity and specificity, the AUC, and the 95%
confidence interval for the AUC. To compare ROC curves between selected models,
we use two-sided, stratified bootstrap tests for the difference between the AUC
values (Robin et al., 2011).

```{r save-predicted-values}
# Save model predicted values to facilitate computing classification measures.
VTAData <- VTAData %>% 
  mutate(pred.m1a = predict(m1a, type = "response"),
         pred.m1b = predict(m1b, type = "response"),
         pred.m2a = predict(m2a, type = "response"),
         pred.m2b = predict(m2b, type = "response"),
         pred.m3a = predict(m3a, type = "response"),
         pred.m3b = predict(m3b, type = "response"))
```

Finally, we run model diagnostics to see if there are any obvious problems with
each model. First we check for outliers, examine residual plots and index plots
for a couple influence measures to look for observations with values exceeding 
the cutoffs.

\FloatBarrier

# Model 1a: Parallel Course Effect 
We first fit a basic model that omits the intercept term in order to
simplify post-processing of the model results into interpretable estimates. We
focus here on a model examining whether the course a learner is taking affects
the pass rate for each level transition testlet. This model assumes that the
higher level courses have a constant effect on the pass rates across all level
transition testlets. Table \ref{tab:m1a-coef} below shows the raw parameter
estimates, confidence intervals, s-values, BFBs, and posterior probabilities of
H1 corresponding to the p-values.

```{r m1a-coef}
m1a %>% 
  tidy(., conf.int = TRUE, conf.level = .95) %>% 
  cbind(., convertp(.$p.value, digits = 2)) %>% 
  kable(format = "latex", booktabs = TRUE, format.args = list(digits = 3), 
        digits = c(2, 2, 2, 2, Inf, 2, 2, 2, 2, 2), 
        col.names = c("Term", "Estimate", "SE", "z-value", "p-value", "CI.LL", 
                      "CI.UL", "S", "BFB", "PPH1"),
        caption = "Model 1a Coefficients")
```

\FloatBarrier

## Sequential Tests (Type I SS)
Each row in Table \ref{tab:m1a-Type-I} tests the significance of unique 
additional variance explained by the term on that line after controlling for all
previously entered terms. Significant results mean adding that term improved the
model.

```{r m1a-Type-I}
m1a %>%
 anova(., test = "Chisq") %>%
 cbind(., convertp(.[,"Pr(>Chi)"])) %>%
 kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 2, 0, 2, Inf, 2, 2, 2),
       col.names = c("DF", "Deviance", "Resid. DF", "Resid. Dev", "p-value",
                     "S", "BFB", "PPH1"),
       caption = "Model 1a Sequential Tests (Type I SS): Analysis of Deviance")
```

\FloatBarrier

## Simultaneous Tests of Main Effects via LRT (Type III SS)
The simultaneous tests in Table \ref{tab:m1a-Type-III} are the effects of the
indicated terms after controlling for all other terms in the model. 

```{r m1a-Type-III}
m1a %>% 
  drop1(., test = "Chisq") %>%
  cbind(., convertp(.[,"Pr(>Chi)"])) %>% 
  kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 2, 2, 2, Inf, 2, 2, 2),
       col.names = c("DF", "Deviance", "AIC", "LRT", "p-value", "S", "BFB",
                     "PPH1"),
       caption = "Model 1a Simultaneous Tests (Type III SS)")
```

\FloatBarrier

## Conditional and Unconditional Pass Rates
Table \ref{tab:m1a-pass-rates} shows the conditional and unconditional pass
rates estimated by Model 1a as a function of level transition testlet and
course.

```{r m1a-pass-rates}
# Create a new data frame object for use with predict()
m1a.ND <- data.frame(Testlet = gl(n = 4, k = 1, length = 16,
                                  labels = c("1", "2", "3", "4")), 
                     Course = gl(n = 4, k = 4, length = 16, 
                                 labels = c("100", "200", "300", "400")))

# Compute predicted mean passing rate at each combination of Level & Course
m1a.pred <- predict(m1a, newdata = m1a.ND, type = "link", se.fit = TRUE)

# Add fitted values and CIs to the new data frame & display it.
critval       <- qnorm(0.975)  # For Wald 95% CIs
m1a.ND$fit    <- m1a.pred$fit
m1a.ND$se.fit <- m1a.pred$se.fit
m1a.ND$fit.LL <- with(m1a.ND, fit - (critval * se.fit))
m1a.ND$fit.UL <- with(m1a.ND, fit + (critval * se.fit))

# Convert fitted values and CIs to probabilities. 
m1a.ND$Pass.Rate <- invlogit(m1a.ND$fit)
m1a.ND$Pass.LL   <- invlogit(m1a.ND$fit.LL)
m1a.ND$Pass.UL   <- invlogit(m1a.ND$fit.UL)

# Compute unconditional pass rates.
m1a.ND$Pass.URate <- c(cumprod(m1a.ND[m1a.ND$Course == "100", "Pass.Rate"]),
                       cumprod(m1a.ND[m1a.ND$Course == "200", "Pass.Rate"]),
                       cumprod(m1a.ND[m1a.ND$Course == "300", "Pass.Rate"]),
                       cumprod(m1a.ND[m1a.ND$Course == "400", "Pass.Rate"]))

ShowVars <- c("Testlet", "Course", "Pass.Rate", "Pass.LL", "Pass.UL",
              "Pass.URate")
kable(m1a.ND[, ShowVars], format = "latex", booktabs = TRUE, digits = 2, 
      format.args = list(nsmall = 2),
      caption = paste("Conditional Pass Rates with 95 percent CIs and",
                      "Unconditional Pass Rates by Level Transition and Course")) %>%
kable_styling(latex_options = c("repeat_header"))
```

\FloatBarrier

## Odds-Ratios for Course Effect
Table \ref{tab:m1a-ORs-Course} shows the Model 1a odds-ratios for the effect of
being in a second-, third-, or fourth-year Spanish course instead of a
first-year course. They quantify the course effect sizes, which this model
assumes are equal across level transition testlets. Model 1b tests whether that
assumption is reasonable.

```{r m1a-ORs-Course}
# Run multiple comparisons examine the course effect & get adjusted 95% CIs. 
m1a.ct <- glht(m1a, linfct = mcp(Course = "Tukey"))
m1a.mc <- summary(m1a.ct, test = adjusted("Westfall"))
m1a.ci <- confint(m1a.ct, calpha = adjusted_calpha(test = "Westfall"))
data.frame(Est   = m1a.mc$test$coefficients, 
           SE    = m1a.mc$test$sigma, 
           CI.LL = m1a.ci$confint[, "lwr"],
           CI.UL = m1a.ci$confint[, "upr"],
           OR    = exp(m1a.mc$test$coefficients),
           OR.LL = exp(m1a.ci$confint[, "lwr"]),
           OR.UL = exp(m1a.ci$confint[, "upr"]),
           z     = m1a.mc$test$tstat,
           p     = m1a.mc$test$pvalues,
           Sval  = p2s(m1a.mc$test$pvalues),
           BFB   = p2bfb(m1a.mc$test$pvalues),
           PPH1  = p2pp(m1a.mc$test$pvalues)) %>% 
  kable(., format = "latex", booktabs = TRUE, format.args = list(digits = 3),
        digits = c(rep(x = 2, times = 8), Inf, 2, 2, 2), 
        caption = paste("Multiple Comparisons for Course Effect, Westfall",
                        "(1997) Adjustment for Multiplicity")) %>%
  kable_styling(latex_options = c("repeat_header"))
```

\FloatBarrier

## Assessing Goodness of Fit, Discrimination, and Calibration

\FloatBarrier

### Hosmer-Lemeshow Goodness of Fit Test
Figure \ref{fig:m1a-calib-plot} shows a calibration plot for this model. The 
plot is based on the bins summarized in Table \ref{tab:m1a-bins}, while the
Hosmer-Lemeshow test is shown in Table \ref{tab:m1a-HLT}. 

```{r m1a-calib-plot, message=FALSE, fig.height=4, fig.width=6, fig.cap=FCap}
FCap <- paste("\\label{fig:m1a-calib-plot}",
              "Model 1a Calibration Plot with Bin Sample Sizes and",
              "Hosmer-Lemeshow Test")
HLT.m1a <- HLfit(m1a, bin.method = "prob.bins", min.prob.interval = 0.1, 
                 xlab = "Predicted Probability",
                 ylab = "Observed Probability")
```

```{r m1a-bins}
HLT.bin.vnames <- c("Bin Center (Median)", "Bin N", 
                    "Observed Proportion", "Predicted Proportion",
                    "Lower Limit", "Upper Limit")
FN <- "Minimum bin interval width = 0.10."

HLT.m1a$bins.table %>% 
  kable(., format = "latex", booktabs = TRUE, digits = 3, row.names = FALSE,
        col.names = HLT.bin.vnames,
        caption = "Calibration Bins Used for Model 1a Hosmer-Lemeshow Test") %>% 
  add_header_above(., header = c(" " = 4, "Obs. Prop. 95% CI" = 2)) %>% 
  column_spec(column = 1, width = "1.75cm") %>% 
  column_spec(column = 3:4, width = "1.75cm") %>% 
  column_spec(column = 5:6, width = "1.25cm") %>% 
  footnote(general = FN, general_title = "Note: ", footnote_as_chunk = TRUE,
         threeparttable = TRUE)
```

```{r m1a-HLT}
HLT.col.vnames <- c("Chi-square", "df", "p", "RMSE", "S", "BFB", "PPH1")
FN <- "Minimum bin interval width = 0.10."

HLT.m1a %>% 
  as_tibble() %>%  
  select(chi.sq, DF, p.value, RMSE) %>% 
  unique() %>% 
  cbind(., convertp(p = .$p.value, digits = 2)) %>% 
  kable(., format = "latex", booktabs = TRUE, 
        digits = c(2, 0, Inf, 2, 2, 2, 2),
        col.names = HLT.col.vnames,
        caption = "Hosmer-Lemeshow Test for Goodness of Fit of Model 1a") %>% 
  footnote(general = FN, general_title = "Note: ", footnote_as_chunk = TRUE,
         threeparttable = TRUE)
```

\FloatBarrier

### Classification 
Table \ref{tab:m1a-classification} presents the classification measures for this
model.

```{r m1a-classification}
set.seed(4921) # For reproducibility of bootstrap estimates.
roc.m1a <- roc(Pass ~ pred.m1a, data = VTAData, ci = TRUE, direction = "<",
               ci.method = "bootstrap", boot.n = 10000)
round(lrcm(roc.m1a, seed = 4684), digits = 3) %>% 
  kable(format = "latex", booktabs = TRUE, 
        caption = "Classification Measures for Model 1a") %>% 
  add_header_above(header = c(" " = 2, "Bootstrapped Quantiles" = 3))
```

\FloatBarrier

### Area Under the Curve (AUC) 
Figure \ref{fig:m1a-auc-plot} shows the ROC curve for the model, annotated with 
the best classification threshold for balancing sensitivity versus specificity 
and the area under the curve (AUC). 

```{r m1a-auc-plot, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m1a-auc-plot}",
              "Model 1a Receiver Operating Characteristic (ROC) Curve.",
              "The dot marks the best classification threshold.",
              "AUC 95% confidence interval obtained via stratified bootstrap",
              "with 10,000 replicates.")
print(roc.m1a)
plot.roc(roc.m1a, print.auc = TRUE, print.auc.cex = .8, print.thres = "best",
         print.thres.cex = .8)
```

Based on this AUC result, we conclude that Model 1a is generally poor at
discriminating people who are passing versus failing the level transitions. That
means course is not a particularly good predictor regardless of what hypothesis
testing results show.

\FloatBarrier

### $R^{2}$ Measures
The low values for $R^{2}_p = `r pseudoR2(m1a, digits = 2)`$ and 
$R^{2}_{Dev} = `r R2Dev(m1a, digits = 2)`$ are not very encouraging about this 
model. 

\FloatBarrier

## Diagnostics   
We need to run model diagnostics to see if there are any obvious problems with
the model. First we check for outliers and find that there are none of note.

```{r m1a-outlier-test}
outlierTest(m1a)
```

Figure \ref{fig:m1a-resid-plots} shows residual plots for this model. 

```{r m1a-resid-plots, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m1a-resid-plots}",
              "Model 1a Residual Plots.")
residualPlots(m1a, layout = c(1,3))
```

Now we look at some index plots for influence measures. We are looking for
observations with values exceeding the cutoffs. Figure \ref{fig:m1a-plotCookD}
shows an index plot of Cook's D for this model.

```{r m1a-plotCookD, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m1a-plotCookD}",
              "Model 1a Index Plot of Coook's D.")
PlotCookD(m1a)
```

```{r m1a-high-CookD}
# How many cases have high Cook's Distances (D > cutoff)?
summary(cooks.distance(m1a))
addmargins(table(cooks.distance(m1a) > CookDco(m1a)))
```

Figure \ref{fig:m1a-plot-leverage} shows an index plot of leverage values for 
this model, while Figure \ref{fig:m1a-plot-Pearson-leverage} shows the 
standardized residuals versus the leverage values, with contours for Cook's 
D. 

```{r m1a-plot-leverage, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m1a-plot-leverage}",
              "Model 1a Index Plot of Leverage Values.")
PlotHat(m1a)
```

```{r m1a-plot-Pearson-leverage, fig.height=4, fig.width=6, fig.cap=FCap}
FCap <- paste("\\label{fig:m1a-plot-Pearson-leverage}",
              "Model 1a Plot of Standardized Pearson Residuals Versus", 
              "Leverage Values, with Cook's D Contours.")
plot(m1a, which = 5, id.n = 10, cex.id = .6, caption = "", sub.caption = "",
     cook.levels = round(CookDco(m1a), digits = 3))
abline(v = hatco(m1a), lty = 2, col = "blue")
text(x = hatco(m1a), y = 3, pos = 4, col = "blue", cex = .75,
     labels = paste("Leverage cutoff >", round(hatco(m1a), digits = 3)))
```

```{r m1a-high-leverage}
# How many cases have high leverage (hat > cutoff)?
summary(hatvalues(m1a))
addmargins(table(hatvalues(m1a) > hatco(m1a)))
```

```{r m1a-influential-cases}
# Identify cases w/ high leverage and high Cook's D. 
InfCases(m1a)
```

There are a lot of observations with high Cook's distance and high leverage.
That is a bit concerning, but this may not be the final model we want to use for
drawing conclusions. 

\FloatBarrier

## Graphs

\FloatBarrier

### Main Effect of Testlet
Figures \ref{fig:m1a-plot-Testlet-LogOdds} and \ref{fig:m1a-plot-Testlet-CPR}
respectively plot the main effect of the self-assessment level transition 
testlet on the scale of the linear predictor (log-odds) and on the response 
scale (probability or conditional pass rates). 

```{r m1a-plot-Testlet-LogOdds, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m1a-plot-Testlet-LogOdds}",
              "Model 1a Main Effect of Testlet on Log-Odds Scale.")
visreg(m1a, xvar = "Testlet", ylab = "Log odds (Pass)")
```

```{r m1a-plot-Testlet-CPR, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m1a-plot-Testlet-CPR}",
              "Model 1a Main Effect of Testlet on Probability Scale.")
visreg(m1a, xvar = "Testlet", ylab = "Conditional Pass Rate", 
       scale = "response", rug = 2, ylim = c(0, 1))
```

### Main Effect of Course
Figures \ref{fig:m1a-plot-Course-LogOdds} and \ref{fig:m1a-plot-Course-CPR}
respectively plot the main effect of course on the scale of the linear predictor
(log-odds) and on the response scale (probability or conditional pass rates).

```{r m1a-plot-Course-LogOdds, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m1a-plot-Course-LogOdds}",
              "Model 1a Main Effect of Course on Log-Odds Scale.")
visreg(m1a, xvar = "Course", ylab = "Log odds (Pass)")
```

```{r m1a-plot-Course-CPR, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m1a-plot-Course-CPR}",
              "Model 1a Main Effect of Course on Probability Scale.")
visreg(m1a, xvar = "Course", ylab = "Conditional Pass Rate", 
       scale = "response", rug = 2, ylim = c(0, 1))
```

\FloatBarrier

### Conditional Pass Rates 
Figure \ref{fig:m1a-plot-CPR} plots the conditional pass rates derived from
Model 1a as a function of both testlet and course.

```{r m1a-plot-CPR, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m1a-plot-CPR}",
              "Model 1a Conditional Pass Rates by Self-Assessment Testlet and",
              "Course.",
              "Lines are labeled with Course.")
m1a.FCPR <- ggplot(m1a.ND[, ShowVars], 
                   aes(Testlet, Pass.Rate, group=Course, color=Course))+
      geom_line()+ coord_cartesian(ylim = c(0, 0.8))+
      xlab("Self-Assessment Testlet")+
      ylab("Conditional Pass Rate")
direct.label(m1a.FCPR, "first.points")
```

\FloatBarrier

### Unconditional Pass Rates
Figure \ref{fig:m1a-plot-UPR} plots the unconditional pass rates derived from
Model 1a as a function of both testlet and course.

```{r m1a-plot-UPR, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m1a-plot-UPR}",
              "Model 1a Unconditional Pass Rates by Self-Assessment Testlet",
              "and Course.",
              "Lines are labeled with Course.")
m1a.FUPR <- ggplot(m1a.ND[, ShowVars], 
                   aes(Testlet, Pass.URate, group=Course, color=Course))+
      geom_line()+ coord_cartesian(ylim = c(0, 0.65))+
      xlab("SA Level Transition")+
      ylab("Unconditional Pass Rate")
direct.label(m1a.FUPR, "first.points")
```

\FloatBarrier

## Conclusion
Learners taking second-, third-, and fourth-year courses are more likely to pass
each of the level transitions than learners in the first-year courses. The
increasing trend in the odds-ratios across the more advanced courses indicate
that the more advanced the course, the larger the difference in the passing
rates is relative to first-year courses. This makes intuitive sense: we expect
learners with more training to do better on self-assessments if the instrument
is a valid measure of Spanish proficiency.

The course effects in this model are assumed to be parallel (constant across the
different level transitions), but we can test whether that assumption needs to
be relaxed by running another model and comparing it to this one.

Despite the significant effects, the model doesn't discriminate well between
those who pass vs. fail (it has modest accuracy, low sensitivity, high
specificity, and low AUC). It explains only a small percentage of the variance.
We can probably do better by considering another predictor. 

\FloatBarrier

# Model 1b: Non-parallel Course Effect
We again omit the intercept term in order to simplify post-processing of the
model results into interpretable estimates. We continue to focus on a model
examining whether the course a learner is taking affects the pass rate, but
relax the assumption that it has a constant effect across all level transitions.
This model allows course to have a level-specific effect on the pass rates.
Table \ref{tab:m1b-coef} below shows the raw parameter estimates, confidence
intervals, s-values, BFBs, and posterior probabilities of H1 corresponding to
the p-values.

```{r m1b-coef}
m1b %>% 
  tidy(., conf.int = TRUE, conf.level = .95) %>% 
  cbind(., convertp(.$p.value, digits = 2)) %>% 
  kable(format = "latex", booktabs = TRUE, format.args = list(digits = 3), 
        digits = c(2, 2, 2, 2, Inf, 2, 2, 2, 2, 2), 
        col.names = c("Term", "Estimate", "SE", "z-value", "p-value", "CI.LL", 
                      "CI.UL", "S", "BFB", "PPH1"),
        caption = "Model 1b Coefficients")
```

\FloatBarrier

## Sequential Tests (Type I SS)
Here, we want to focus on the result for the interaction effect. That tells us
whether allowing course to have a level-specific effect instead of a constant
effect across levels improved the model.

```{r m1b-Type-I}
m1b %>%
 anova(., test = "Chisq") %>%
 cbind(., convertp(.[,"Pr(>Chi)"])) %>%
 kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 2, 0, 2, Inf, 2, 2, 2),
       col.names = c("DF", "Deviance", "Resid. DF", "Resid. Dev", "p-value",
                     "S", "BFB", "PPH1"),
       caption = "Model 1b Sequential Tests (Type I SS): Analysis of Deviance")
```

\FloatBarrier

## Simultaneous Tests of Interaction Effects via LRT (Type III SS)

Another way to test whether the interaction was significant is to compare models
via a likelihood ratio test (LRT). We can do that by feeding the *anova()*
function two model fit objects. The models must be nested (one contains only a
subset of the parameters in the other model).

```{r m1a-m1b-LRT}
anova(m1a, m1b, test = "Chisq") %>%  
  cbind(., convertp(.[,"Pr(>Chi)"])) %>% 
  kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 2, 0, 2, Inf, 2, 2, 2),
       col.names = c("Resid. Df", "Resid. Deviance", "Df", "Deviance", 
                     "p-value", "S", "BFB", "PPH1"),
       caption = "Model 1a vs 1b: Likelihood Ratio Test for Interaction")
```

Both methods demonstrated for testing the interaction term yield the same
p-value, so we will only use one of them from here on out.

\FloatBarrier

## Conditional and Unconditional Pass Rates
We omit these rates because Model 1b is not better than Model 1a. 

\FloatBarrier

## Odds-Ratios for Course Effect
We omit these odds-ratios because Model 1b is not better than Model 1a. 

\FloatBarrier

## Assessing Goodness of Fit, Discrimination, and Calibration
Since we are running the continuation-ration model as a logistic regression, all
the stadard methods for assessing goodness of fit for logistic regresion models
apply.

\FloatBarrier

### Hosmer-Lemeshow Goodness of Fit Test
Figure \ref{fig:m1b-calib-plot} shows a calibration plot for this model. The 
plot is based on the bins summarized in Table \ref{tab:m1b-bins}, while the
Hosmer-Lemeshow test is shown in Table \ref{tab:m1b-HLT}. 

```{r m1b-calib-plot, message=FALSE, fig.height=4, fig.width=6, fig.cap=FCap}
FCap <- paste("\\label{fig:m1b-calib-plot}",
              "Model 1b Calibration Plot with Bin Sample Sizes and",
              "Hosmer-Lemeshow Test")
HLT.m1b <- HLfit(m1b, bin.method = "prob.bins", min.prob.interval = 0.1, 
                 xlab = "Predicted Probability",
                 ylab = "Observed Probability")
```

```{r m1b-bins}
FN <- "Minimum bin interval width = 0.10."

HLT.m1b$bins.table %>% 
  kable(., format = "latex", booktabs = TRUE, digits = 3, row.names = FALSE,
        col.names = HLT.bin.vnames,
        caption = "Calibration Bins Used for Model 1b Hosmer-Lemeshow Test") %>% 
  add_header_above(., header = c(" " = 4, "Obs. Prop. 95% CI" = 2)) %>% 
  column_spec(column = 1, width = "1.75cm") %>% 
  column_spec(column = 3:4, width = "1.75cm") %>% 
  column_spec(column = 5:6, width = "1.25cm") %>% 
  footnote(general = FN, general_title = "Note: ", footnote_as_chunk = TRUE,
         threeparttable = TRUE)
```

```{r m1b-HLT}
FN <- "Minimum bin interval width = 0.10."

HLT.m1b %>% 
  as_tibble() %>%  
  select(chi.sq, DF, p.value, RMSE) %>% 
  unique() %>% 
  cbind(., convertp(p = .$p.value, digits = 2)) %>% 
  kable(., format = "latex", booktabs = TRUE, 
        digits = c(2, 0, Inf, 2, 2, 2, 2),
        col.names = HLT.col.vnames,
        caption = "Hosmer-Lemeshow Test for Goodness of Fit of Model 1b") %>% 
  footnote(general = FN, general_title = "Note: ", footnote_as_chunk = TRUE,
         threeparttable = TRUE)
```

\FloatBarrier

### Classification 
Table \ref{tab:m1b-classification} presents the classification measures for this
model.

```{r m1b-classification}
set.seed(1574) # For reproducible bootstrap estimates.
roc.m1b <- roc(Pass ~ pred.m1b, data = VTAData, ci = TRUE, direction = "<",
               ci.method = "bootstrap", boot.n = 10000)
round(lrcm(roc.m1b, seed = 6711), digits = 3) %>% 
  kable(format = "latex", booktabs = TRUE, 
        caption = "Classification Measures for Model 1b") %>% 
  add_header_above(header = c(" " = 2, "Bootstrapped Quantiles" = 3))
```

The low sensitivity indicates that the model does a poor job of correctly
identifying those who pass the level transitions. The high specificity indicates
that it does a good job of identifying those who fail to pass level transitions.

\FloatBarrier

### Area Under the Curve (AUC)
Figure \ref{fig:m1b-auc-plot} shows the ROC curve for the model, annotated with 
the best classification threshold for balancing sensitivity versus specificity 
and the area under the curve (AUC). 

```{r m1b-auc-plot, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m1b-auc-plot}",
              "Model 1b Receiver Operating Characteristic (ROC) Curve.",
              "The dot marks the best classification threshold.",
              "AUC 95% confidence interval obtained via stratified bootstrap",
              "with 10,000 replicates.")
print(roc.m1b)
plot.roc(roc.m1b, print.auc = TRUE, print.auc.cex = .8, print.thres = "best",
         print.thres.cex = .8)
```

Based on this AUC result, we conclude that Model 1b does a poor job of
discriminating between those who pass vs fail the level transitions. That means
course is still not a particularly good predictor even after we allow it to have
a non-parallel effect. Table \ref{tab:m1a-m1b-auc-test} compares the ROC curves
for Models 1a and 1b and shows that they do not differ meaningfully with respect
to discrimination performance. 

```{r m1a-m1b-auc-test, message=FALSE}
set.seed(9537) # For reproducible bootstrap estimates.
roct.m1a.m1b <- roc.test(roc.m1a, roc.m1b, method = "bootstrap", boot.n = 10000)
glance(roct.m1a.m1b) %>% 
  cbind(., convertp(p = .$p.value, digits = 2)) %>% 
  select(boot.n, estimate1, estimate2, statistic, p.value, S, BFB, PPH1) %>% 
  kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 3, 3, 2, Inf, 2, 2, 2),
       col.names = c("Bootstrap N", "Model 1a", "Model 1b", "D", "p", "S", 
                     "BFB", "PPH1"),
       caption = paste("Comparing Models 1a and 1b Via Two-Sided, Stratified",
                       "Bootstrap Test for Correlated ROC Curves")) %>% 
  add_header_above(header = c(" ", "AUC Estimates" = 2, " " = 5))
```

\FloatBarrier

### $R^{2}$ Measures
The results for Model 1b are only marginally better than they were for Model 1a: 
$R^{2}_p = `r pseudoR2(m1b, digits = 2)`$ and 
$R^{2}_{Dev} = `r R2Dev(m1b, digits = 2)`$. 

\FloatBarrier

## Diagnostics
We omit detailed diagnostics because Model 1b is not better than Model 1a. 

\FloatBarrier

## Graphs
We omit graphs because Model 1b is not better than Model 1a. See Model 1a graphs
instead.

\FloatBarrier

## Conclusion
The interaction is not improving the model. We should interpret Model 1a instead
of Model 1b. That may change if we build in other covariates. However, we should
also test COPIC as an alternative to course. We turn to that next in Model 2a.

\FloatBarrier

# Model 2a: Parallel OPIc Effect 
We now want to look at a different covariate, namely OPIc speaking proficiency
scores. We again fit a model that omits the intercept term in order to simplify
post-processing of the model results into interpretable estimates. We start by
assuming that the OPIc scores have a constant effect on the pass rates across
all level transitions. We are using the centered version of OPIc in the model.
Table \ref{tab:m2a-coef} below shows the raw parameter estimates, confidence
intervals, s-values, BFBs, and posterior probabilities of H1 corresponding to
the p-values.


```{r m2a-coef}
m2a %>% 
  tidy(., conf.int = TRUE, conf.level = .95) %>% 
  cbind(., convertp(.$p.value, digits = 2)) %>% 
  kable(format = "latex", booktabs = TRUE, format.args = list(digits = 3), 
        digits = c(2, 2, 2, 2, Inf, 2, 2, 2, 2, 2), 
        col.names = c("Term", "Estimate", "SE", "z-value", "p-value", "CI.LL", 
                      "CI.UL", "S", "BFB", "PPH1"),
        caption = "Model 2a Coefficients")
```

\FloatBarrier

## Sequential Tests (Type I SS)
Each row in Table \ref{tab:m2a-Type-I} tests the significance of unique
additional variance explained by the term on that line after controlling for all
previously entered terms. Significant results mean adding that term improved the
model.

```{r m2a-Type-I}
m2a %>%
 anova(., test = "Chisq") %>%
 cbind(., convertp(.[,"Pr(>Chi)"])) %>%
 kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 2, 0, 2, Inf, 2, 2, 2),
       col.names = c("DF", "Deviance", "Resid. DF", "Resid. Dev", "p-value",
                     "S", "BFB", "PPH1"),
       caption = "Model 2a Sequential Tests (Type I SS): Analysis of Deviance")
```

\FloatBarrier

## Simultaneous Tests of Main Effects via LRT (Type III SS)
The simultaneous tests in Table \ref{tab:m2a-Type-III} are the effects of the
indicated terms after controlling for all other terms in the model. They are
only computed for terms that are not part of a higher-order interaction because
it makes no sense to test for a main effect when the variable is involved in an
interaction. These should be functionally equivalent to the results you get with
*anova()* when you feed it a pair of nested models that differ only in that one
model includes a term that is absent from the other model. They are likelihood
ratio tests (LRTs).

```{r m2a-Type-III}
m2a %>% 
  drop1(., test = "Chisq") %>%
  cbind(., convertp(.[,"Pr(>Chi)"])) %>% 
  kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 2, 2, 2, Inf, 2, 2, 2),
       col.names = c("DF", "Deviance", "AIC", "LRT", "p-value", "S", "BFB",
                     "PPH1"),
       caption = "Model 2a Simultaneous Tests (Type III SS)")
```

\FloatBarrier

## Conditional and Unconditional Pass Rates
Table \ref{tab:m2a-pass-rates} shows the conditional and unconditional pass
rates estimated by Model 1a as a function of level transition testlet and
COPIC scores. 

```{r m2a-pass-rates}
# Create a new data frame object for use with predict()
m2a.ND <- data.frame(Testlet = gl(n = 4, k = 1, length = 36, 
                                  labels = c("1", "2", "3", "4")),
                     COPIC = rep(-4:4, each = 4), OPIC = rep(1:9, each = 4))

# Compute predicted mean passing rate at each combination of Level & Course
m2a.pred <- predict(m2a, newdata = m2a.ND, type = "link", se.fit = TRUE)

# Add fitted values and CIs to the new data frame & display it.
critval       <- qnorm(0.975) # For Wald 95% CIs
m2a.ND$fit    <- m2a.pred$fit
m2a.ND$se.fit <- m2a.pred$se.fit
m2a.ND$fit.LL <- with(m2a.ND, fit - (critval * se.fit))
m2a.ND$fit.UL <- with(m2a.ND, fit + (critval * se.fit))

# Convert fitted values and CIs to probabilities. 
m2a.ND$Pass.Rate <- invlogit(m2a.ND$fit)
m2a.ND$Pass.LL   <- invlogit(m2a.ND$fit.LL)
m2a.ND$Pass.UL   <- invlogit(m2a.ND$fit.UL)

# Compute unconditional pass rates.
m2a.ND$Pass.URate <- c(cumprod(m2a.ND[m2a.ND$COPIC == -4, "Pass.Rate"]),
                       cumprod(m2a.ND[m2a.ND$COPIC == -3, "Pass.Rate"]),
                       cumprod(m2a.ND[m2a.ND$COPIC == -2, "Pass.Rate"]),
                       cumprod(m2a.ND[m2a.ND$COPIC == -1, "Pass.Rate"]),
                       cumprod(m2a.ND[m2a.ND$COPIC == 0, "Pass.Rate"]),
                       cumprod(m2a.ND[m2a.ND$COPIC == 1, "Pass.Rate"]),
                       cumprod(m2a.ND[m2a.ND$COPIC == 2, "Pass.Rate"]),
                       cumprod(m2a.ND[m2a.ND$COPIC == 3, "Pass.Rate"]),
                       cumprod(m2a.ND[m2a.ND$COPIC == 4, "Pass.Rate"]))

ShowVars <- c("Testlet", "COPIC", "OPIC", "Pass.Rate", "Pass.LL", "Pass.UL",
              "Pass.URate")
 
kable(m2a.ND[, ShowVars], format = "latex", booktabs = TRUE, digits = 2, 
      format.args = list(nsmall = 2),
      caption = paste("Conditional Pass Rates (with 95 percent CIs) and",
                      "Unconditional Pass Rates by Level Transition and Course")) %>% 
kable_styling(latex_options = c("repeat_header")) 
```

\FloatBarrier

## Odds-Ratio for COPIC Effect 
We exponentiate the COPIC parameter to obtain the odds-ratio showing the effect
of an extra point on the COPIC (i.e., a score of 6 vs 5 in the raw variable).
This quantifies the effect of COPIC, which this model assumes is equal across
level transitions. We will test whether that assumption is reasonable in another
model later.

```{r m2a-OR-COPIC}
m2a.OR <- cbind(OR = exp(coef(m2a)[5]), 
                LL = exp(confint(m2a, level = 0.95)[5, 1]),
                UL = exp(confint(m2a, level = 0.95)[5, 2]))
dimnames(m2a.OR)[[1]] <- "COPIC (0 vs 1)"
kable(m2a.OR, format = "latex", booktabs = TRUE, digits = 2, 
      caption = paste("Odds-Ratios for COPIC Effect (Estimates and 95 Percent",
                      "Confidence Intervals)")) %>% 
  kable_styling(latex_options = c("repeat_header")) 
```

\FloatBarrier

## Assessing Goodness of Fit, Discrimination, and Calibration
Since we are running the continuation-ration model as a logistic regression, all
the stadard methods for assessing goodness of fit for logistic regresion models
apply.

\FloatBarrier

### Hosmer-Lemeshow Goodness of Fit Test
Figure \ref{fig:m2a-calib-plot} shows a calibration plot for this model. The 
plot is based on the bins summarized in Table \ref{tab:m2a-bins}, while the
Hosmer-Lemeshow test is shown in Table \ref{tab:m2a-HLT}. 

```{r m2a-calib-plot, message=FALSE, fig.height=4, fig.width=6, fig.cap=FCap}
FCap <- paste("\\label{fig:m2a-calib-plot}",
              "Model 2a Calibration Plot with Bin Sample Sizes and",
              "Hosmer-Lemeshow Test")
HLT.m2a <- HLfit(m2a, bin.method = "prob.bins", min.prob.interval = 0.1, 
                 xlab = "Predicted Probability",
                 ylab = "Observed Probability")
```

```{r m2a-bins}
FN <- "Minimum bin interval width = 0.10."

HLT.m2a$bins.table %>% 
  kable(., format = "latex", booktabs = TRUE, digits = 3, row.names = FALSE,
        col.names = HLT.bin.vnames,
        caption = "Calibration Bins Used for Model 2a Hosmer-Lemeshow Test") %>% 
  add_header_above(., header = c(" " = 4, "Obs. Prop. 95% CI" = 2)) %>% 
  column_spec(column = 1, width = "1.75cm") %>% 
  column_spec(column = 3:4, width = "1.75cm") %>% 
  column_spec(column = 5:6, width = "1.25cm") %>% 
  footnote(general = FN, general_title = "Note: ", footnote_as_chunk = TRUE,
         threeparttable = TRUE)
```

```{r m2a-HLT}
FN <- "Minimum bin interval width = 0.10."

HLT.m2a %>% 
  as_tibble() %>%  
  select(chi.sq, DF, p.value, RMSE) %>% 
  unique() %>% 
  cbind(., convertp(p = .$p.value, digits = 2)) %>% 
  kable(., format = "latex", booktabs = TRUE, 
        digits = c(2, 0, Inf, 2, 2, 2, 2),
        col.names = HLT.col.vnames,
        caption = "Hosmer-Lemeshow Test for Goodness of Fit of Model 2a") %>% 
  footnote(general = FN, general_title = "Note: ", footnote_as_chunk = TRUE,
         threeparttable = TRUE)
```

\FloatBarrier

### Classification 
Table \ref{tab:m2a-classification} presents the classification measures for 
this model. 

```{r m2a-classification}
set.seed(5378) # For reproducible bootstrap estimates.
roc.m2a <- roc(Pass ~ pred.m2a, data = VTAData, ci = TRUE, direction = "<",
               ci.method = "bootstrap", boot.n = 10000)
round(lrcm(roc.m2a, seed = 9825), digits = 3) %>% 
  kable(format = "latex", booktabs = TRUE, 
        caption = "Classification Measures for Model 2a") %>% 
  add_header_above(header = c(" " = 2, "Bootstrapped Quantiles" = 3))
```

\FloatBarrier

### Area Under the Curve (AUC)
Figure \ref{fig:m2a-auc-plot} shows the ROC curve for the model, annotated with 
the best classification threshold for balancing sensitivity versus specificity 
and the area under the curve (AUC). Model 2a achieves adequate 

```{r m2a-auc-plot, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2a-auc-plot}",
              "Model 2a Receiver Operating Characteristic (ROC) Curve.",
              "The dot marks the best classification threshold.",
              "AUC 95% confidence interval obtained via stratified bootstrap",
              "with 10,000 replicates.")
print(roc.m2a)
plot.roc(roc.m2a, print.auc = TRUE, print.auc.cex = .8, print.thres = "best",
         print.thres.cex = .8)
```

Model 2a shows acceptable ability to discriminate those who pass from those who
fail to pass the level transition testlets. Table \ref{tab:m1a-m2a-auc-test}
compares the ROC curves for Models 1a and 2a and shows that Model 2a performs 
better than Model 1a with respect to that.

```{r m1a-m2a-auc-test, message=FALSE}
set.seed(7436) # For reproducible bootstrap estimates.
roct.m1a.m2a <- roc.test(roc.m1a, roc.m2a, method = "bootstrap", boot.n = 10000)
glance(roct.m1a.m2a) %>% 
  cbind(., convertp(p = .$p.value, digits = 2)) %>% 
  select(boot.n, estimate1, estimate2, statistic, p.value, S, BFB, PPH1) %>% 
  kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 3, 3, 2, Inf, 2, 2, 2),
       col.names = c("Bootstrap N", "Model 1a", "Model 2a", "D", "p", "S", 
                     "BFB", "PPH1"),
       caption = paste("Comparing Models 1a and 2a Via Two-Sided, Stratified",
                       "Bootstrap Test for Correlated ROC Curves")) %>% 
  add_header_above(header = c(" ", "AUC Estimates" = 2, " " = 5))
```

\FloatBarrier

### $R^{2}$ Measures
The values for $R^{2}_p = `r pseudoR2(m2a, digits = 2)`$ and 
$R^{2}_{Dev} = `r R2Dev(m2a, digits = 2)`$ are more encouraging than those from 
models 1a and 1b. 

## Diagnostics
We need to run model diagnostics to see if there are any obvious problems with 
the model. First we check for outliers and find that there are none of note.

```{r m2a-outlier-test}
outlierTest(m2a)
```

Figure \ref{fig:m2a-resid-plots} shows residual plots for this model. 

```{r m2a-resid-plots, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2a-resid-plots}",
              "Model 2a Residual Plots.")
residualPlots(m2a, layout = c(1,3))
```

Now we look at some index plots for influence measures. We are looking for
observations with values exceeding the cutoffs. Figure \ref{fig:m2a-plotCookD}
shows an index plot of Cook's D for this model.

```{r m2a-plotCookD, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2a-plotCookD}",
              "Model 2a Index Plot of Coook's D.")
PlotCookD(m2a)
```

```{r m2a-high-CookD}
# How many cases have high Cook's Distances (D > cutoff)?
summary(cooks.distance(m2a))
addmargins(table(cooks.distance(m2a) > CookDco(m2a)))
```

Figure \ref{fig:m2a-plot-leverage} shows an index plot of leverage values for 
this model, while Figure \ref{fig:m2a-plot-Pearson-leverage} shows the 
standardized residuals versus the leverage values, with contours for Cook's 
D. 

```{r m2a-plot-leverage, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2a-plot-leverage}",
              "Model 2a Index Plot of Leverage Values.")
PlotHat(m2a)
```

```{r m2a-plot-Pearson-leverage, fig.height=4, fig.width=6, fig.cap=FCap}
FCap <- paste("\\label{fig:m2a-plot-Pearson-leverage}",
              "Model 2a Plot of Standardized Pearson Residuals Versus", 
              "Leverage Values, with Cook's D Contours.")
plot(m2a, which = 5, id.n = 10, cex.id = .6, caption = "", sub.caption = "",
     cook.levels = round(CookDco(m2a), digits = 3))
abline(v = hatco(m2a), lty = 2, col = "blue")
text(x = hatco(m2a), y = 5, pos = 4, col = "blue", cex = .75,
     labels = paste("Leverage cutoff >", round(hatco(m2a), digits = 3)))
```

```{r m2a-high-leverage}
# How many cases have high leverage (hat > cutoff)?
summary(hatvalues(m2a))
addmargins(table(hatvalues(m2b) > hatco(m2a)))
```

```{r m2a-influential-cases}
# Identify cases w/ high leverage and high Cook's D. 
InfCases(m2a)
```

\FloatBarrier

## Graphs

### Main Effect of Testlet
Figures \ref{fig:m2a-plot-Testlet-LogOdds} and \ref{fig:m2a-plot-Testlet-CPR}
respectively plot the main effect of the self-assessment level transition 
testlet on the scale of the linear predictor (log-odds) and on the response 
scale (probability or conditional pass rates). 

```{r m2a-plot-Testlet-LogOdds, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2a-plot-Testlet-LogOdds}",
              "Model 2a Main Effect of Testlet on Log-Odds Scale.")
visreg(m2a, xvar = "Testlet", ylab = "Log odds (Pass)")
```

```{r m2a-plot-Testlet-CPR, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2a-plot-Testlet-CPR}",
              "Model 2a Main Effect of Testlet on Probability Scale.")
visreg(m2a, xvar = "Testlet", ylab = "Conditional Pass Rate", 
       scale = "response", rug = 2, ylim = c(0, 1))
```

### Main Effect of COPIC
Figures \ref{fig:m2a-plot-COPIC-LogOdds} and \ref{fig:m2a-plot-COPIC-CPR}
respectively plot the main effect of COPIC on the scale of the linear predictor
(log-odds) and on the response scale (probability or conditional pass rates).

```{r m2a-plot-COPIC-LogOdds, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2a-plot-COPIC-LogOdds}",
              "Model 2a Main Effect of Centered OPIc Score on Log-Odds Scale.")
visreg(m2a, xvar = "COPIC", jitter = TRUE, scale = "linear", 
       xlab = "Centered OPIC Score (OPIC - 5)", 
       ylab = "Log odds (Pass)")
```

```{r m2a-plot-COPIC-CPR, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2a-plot-COPIC-CPR}",
              "Model 2a Main Effect of Centered OPIc Score on Probability Scale.")
visreg(m2a, xvar = "COPIC", jitter = TRUE, scale = "response", rug = 2,
       xlab = "Centered OPIC Score (OPIC - 5)", 
       ylab = "Conditional Pass Rate")
```

\FloatBarrier

### Conditional Pass Rates
Figure \ref{fig:m2a-plot-CPR} plots the conditional pass rates derived from
Model 2a as a function of both testlet and OPIc score.

```{r m2a-plot-CPR, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2a-plot-CPR}",
              "Model 2a Conditional Pass Rates by OPIc Score and",
              "Self-Assessment Testlet.",
              "Lines are labeled with OPIc score.")
m2a.FCPR <- ggplot(m2a.ND[, ShowVars], 
                   aes(Testlet, Pass.Rate, group=OPIC, color=OPIC))+
  geom_line()+ coord_cartesian(ylim = c(0, 1))+
  xlab("Self-assessment Testlet")+
  ylab("Conditional Pass Rate")
  direct.label(m2a.FCPR, "first.points")
# for the manuscript
ggsave("F4.png", plot=direct.label(m2a.FCPR, "first.points"), 
       width = 6.5, height = 8.5, dpi = 300)
```

\FloatBarrier

### Unconditional Pass Rates  
Figure \ref{fig:m2a-plot-UPR} plots the unconditional pass rates derived from
Model 2a as a function of both testlet and OPIc score.

```{r m2a-plot-UPR, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2a-plot-UPR}",
              "Model 2a Unconditional Pass Rates by OPIc Score and",
              "Self-Assessment Testlet.",
              "Lines are labeled with OPIc score.")
m2a.FUPR <- ggplot(m2a.ND[, ShowVars], 
                   aes(Testlet, Pass.URate, group=OPIC, color=OPIC))+
  geom_line()+ coord_cartesian(ylim = c(0, 1))+
  xlab("Self-assessment Testlet")+
  ylab("Unconditional Pass Rate")
  direct.label(m2a.FUPR, "first.points")
# for the manuscript
ggsave("F5.png", plot=direct.label(m2a.FUPR, "first.points"), 
       width = 9, height = 5.5, dpi = 300)
```

\FloatBarrier

## Conclusion
Model 2a suggests that higher OPIC scores are associated with higher passing
rates across the level transitions. Furthermore, Model 2a is clearly better than
Model 1a. However, we still need to run another model to see whether relaxing
the assumption that the OPIC effect is constant across levels is viable. We do
that in Model 2b.

\newpage

\FloatBarrier

# Model 2b: Non-parallel OPIc Effect 
We now relax the assumption that the OPIC effect is constant across level
transitions. We again fit a model that omits the intercept term in order to
simplify post-processing of the model results into interpretable estimates. We
are still using the centered version of OPIC in the model. Table
\ref{tab:m2b-coef} below shows the raw parameter estimates, confidence
intervals, s-values, BFBs, and posterior probabilities of H1 corresponding to
the p-values.

```{r m2b-coef}
m2b %>% 
  tidy(., conf.int = TRUE, conf.level = .95) %>% 
  cbind(., convertp(.$p.value, digits = 2)) %>% 
  kable(format = "latex", booktabs = TRUE, format.args = list(digits = 3), 
        digits = c(2, 2, 2, 2, Inf, 2, 2, 2, 2, 2), 
        col.names = c("Term", "Estimate", "SE", "z-value", "p-value", "CI.LL", 
                      "CI.UL", "S", "BFB", "PPH1"),
        caption = "Model 2b Coefficients")
```

\newpage

We can see from Table \ref{tab:show-models} that the difference in AICs
($\Delta AIC = AIC_{2a} - AIC_{2b} = `r round(AIC(m2a) - AIC(m2b), digits = 3)`$)
favors Model 2b, which has a lower AIC value than Model 2a, but not by a large
margin. Meanwhile, difference in BICs
($\Delta BIC = BIC_{2a} - BIC_{2b} = `r round(BIC(m2a) - BIC(m2b), digits = 3)`$) 
favors Model 2a by a larger margin because it imposes a stronger penalty for
model complexity (lack of parsimony).

\FloatBarrier

## Sequential Tests (Type I SS)
Each row in Table \ref{tab:m2b-Type-I} tests the significance of unique
additional variance explained by the term on that line after controlling for all
previously entered terms. Significant results mean adding that term improved the
model. Here, we want to focus on the result for the interaction effect. That
tells us whether allowing OPIc to have a level-specific effect instead of a
constant effect across levels improved the model.

```{r m2b-Type-I}
m2b %>%
 anova(., test = "Chisq") %>%
 cbind(., convertp(.[,"Pr(>Chi)"])) %>%
 kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 2, 0, 2, Inf, 2, 2, 2),
       col.names = c("DF", "Deviance", "Resid. DF", "Resid. Dev", "p-value",
                     "S", "BFB", "PPH1"),
       caption = "Model 2b Sequential Tests (Type I SS): Analysis of Deviance")
```

\FloatBarrier

## Simultaneous Tests of Interaction Effects via LRT (Type III SS)
Another way to test whether the interaction was significant is to compare models
via a likelihood ratio test (LRT). We can do that by feeding the *anova()*
function two model fit objects. The models must be nested (one contains only a
subset of the parameters in the other model).

```{r m2a-m2b-LRT}
anova(m2a, m2b, test = "Chisq") %>%  
  cbind(., convertp(.[,"Pr(>Chi)"])) %>% 
  kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 2, 0, 2, Inf, 2, 2, 2),
       col.names = c("Resid. Df", "Resid. Deviance", "Df", "Deviance", 
                     "p-value", "S", "BFB", "PPH1"),
       caption = "Model 2a vs 2b: Likelihood Ratio Test for Interaction")
```

\FloatBarrier

## Conditional and Unconditional Pass Rates
We use the inverse logit transformation to convert fitted values and associated
confidence intervals into the conditional probability of passing a particular
level transition given that the student has a particular COPIC score.

We compute the unconditional pass rates from the conditional pass rates as sets
of cumulative products.

```{r}
# Create a new data frame object for use with predict()
m2b.ND <- data.frame(Testlet = gl(n = 4, k = 1, length = 36,
                                  labels = c("1", "2", "3", "4")), 
                     COPIC = rep(-4:4, each = 4),
                     OPIC = rep(1:9, each = 4))

# Compute predicted mean passing rate at each combination of Level & Course
m2b.pred <- predict(m2b, newdata = m2b.ND, type = "link", se.fit = TRUE)

# Add fitted values and CIs to the new data frame & display it.
critval      <- qnorm(0.975)  # For Wald 95% CIs
m2b.ND$fit    <- m2b.pred$fit
m2b.ND$se.fit <- m2b.pred$se.fit
m2b.ND$fit.LL <- with(m2b.ND, fit - (critval * se.fit))
m2b.ND$fit.UL <- with(m2b.ND, fit + (critval * se.fit))

# Convert fitted values and CIs to probabilities. 
m2b.ND$Pass.Rate <- invlogit(m2b.ND$fit)
m2b.ND$Pass.LL   <- invlogit(m2b.ND$fit.LL)
m2b.ND$Pass.UL   <- invlogit(m2b.ND$fit.UL)

# Compute unconditional pass rates.
m2b.ND$Pass.URate <- c(cumprod(m2b.ND[m2b.ND$COPIC == -4, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == -3, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == -2, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == -1, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == 0, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == 1, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == 2, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == 3, "Pass.Rate"]),
                       cumprod(m2b.ND[m2b.ND$COPIC == 4, "Pass.Rate"]))

ShowVars <- c("Testlet", "COPIC", "OPIC", "Pass.Rate", "Pass.LL", "Pass.UL",
              "Pass.URate")
kable(m2b.ND[, ShowVars], format = "latex", booktabs = TRUE, digits = 2, 
      format.args = list(nsmall = 2),
      caption = paste("Conditional Pass Rates (with 95 percent CIs) and",
                      "Unconditional Pass Rates by Level Transition and Course")) %>% 
kable_styling(latex_options = c("repeat_header")) 
```

\FloatBarrier

## Odds-Ratios for COPIC Effect    
We exponentiate combinations of the parameters to obtain the odds-ratios showing
the effect of COPIC at each level. This quantifies the effect of COPIC, which
this model assumes varies across level transitions. We are using contrasts that
estimate the simple slope of COPIC at each level, then get simultaneous 95% CIs
that are adjusted for multiple testing via Westfall's (1997) method.

```{r}
# Create objects to hold row names and a contrast matrix (K).
RN <- c("Slope @ L1", "Slope @ L2", "Slope @ L3", "Slope @ L4",
        "Slope diff: L1 - L2", "Slope diff: L1 - L3",
        "Slope diff: L1 - L4", "Slope diff: L2 - L3",
        "Slope diff: L2 - L4", "Slope diff: L3 - L4")
K <- matrix(c(0, 0, 0, 0, 1, 0, 0, 0, 
              0, 0, 0, 0, 1, 1, 0, 0, 
              0, 0, 0, 0, 1, 0, 1, 0, 
              0, 0, 0, 0, 1, 0, 0, 1, 
              0, 0, 0, 0, 0, 1, 0, 0, 
              0, 0, 0, 0, 0, 0, 1, 0, 
              0, 0, 0, 0, 0, 0, 0, 1, 
              0, 0, 0, 0, 0, 1, -1, 0, 
              0, 0, 0, 0, 0, 1, 0, -1, 
              0, 0, 0, 0, 0, 0, 1, -1),
            nrow=10, byrow = TRUE,
            dimnames = list(RN, names(coef(m2b))))

# Run multiple comparisons examine the course effect & get adjusted 95% CIs. 
m2b.ct <- glht(m2b, linfct = K)
m2b.mc <- summary(m2b.ct, test = adjusted("Westfall"))
m2b.ci <- confint(m2b.ct, calpha = adjusted_calpha(test = "Westfall"))
T33 <- data.frame(Est   = m2b.mc$test$coefficients, 
                  SE    = m2b.mc$test$sigma, 
                  CI.LL = m2b.ci$confint[, "lwr"],
                  CI.UL = m2b.ci$confint[, "upr"],
                  OR    = exp(m2b.mc$test$coefficients),
                  OR.LL = exp(m2b.ci$confint[, "lwr"]),
                  OR.UL = exp(m2b.ci$confint[, "upr"]),
                  z     = m2b.mc$test$tstat,
                  p     = m2b.mc$test$pvalues,
                  Sval  = p2s(m2b.mc$test$pvalues),
                  BFB   = p2bfb(m2b.mc$test$pvalues),
                  PPH1  = p2pp(m2b.mc$test$pvalues))
T33$OR[5:10]    <- NA
T33$OR.LL[5:10] <- NA
T33$OR.UL[5:10] <- NA
kable(T33, format = "latex", booktabs = TRUE, format.args = list(digits = 3),
      digits = c(rep(x = 2, times = 8), Inf, 2, 2, 2),
      caption = paste("Simple Slopes of COPIC Effect, Westfall (1997)",
                      "Adjustment for Multiplicity")) %>% 
kable_styling(latex_options = c("repeat_header", "scale_down"))
```

\FloatBarrier

## Assessing Assessing Goodness of Fit, Discrimination, and Calibration
Since we are running the continuation-ration model as a logistic regression, all
the stadard methods for assessing goodness of fit for logistic regresion models
apply.

\FloatBarrier

### Hosmer-Lemeshow Goodness of Fit Test
Figure \ref{fig:m2b-calib-plot} shows a calibration plot for this model. The 
plot is based on the bins summarized in Table \ref{tab:m2b-bins}, while the
Hosmer-Lemeshow test is shown in Table \ref{tab:m2b-HLT}. 

```{r m2b-calib-plot, message=FALSE, fig.height=4, fig.width=6, fig.cap=FCap}
FCap <- paste("\\label{fig:m2b-calib-plot}",
              "Model 2b Calibration Plot with Bin Sample Sizes and",
              "Hosmer-Lemeshow Test.")
HLT.m2b <- HLfit(m2b, bin.method = "prob.bins", min.prob.interval = 0.17, 
                 xlab = "Predicted Probability",
                 ylab = "Observed Probability")
```

```{r m2b-bins}
FN <- "Minimum bin interval width = 0.17."

HLT.m2b$bins.table %>% 
  kable(., format = "latex", booktabs = TRUE, digits = 3, row.names = FALSE,
        col.names = HLT.bin.vnames,
        caption = "Calibration Bins Used for Model 2b Hosmer-Lemeshow Test") %>% 
  add_header_above(., header = c(" " = 4, "Obs. Prop. 95% CI" = 2)) %>% 
  column_spec(column = 1, width = "1.75cm") %>% 
  column_spec(column = 3:4, width = "1.75cm") %>% 
  column_spec(column = 5:6, width = "1.25cm") %>% 
  footnote(general = FN, general_title = "Note: ", footnote_as_chunk = TRUE,
         threeparttable = TRUE)
```

```{r m2b-HLT}
FN <- "Minimum bin interval width = 0.17."

HLT.m2b %>% 
  as_tibble() %>%  
  select(chi.sq, DF, p.value, RMSE) %>% 
  unique() %>% 
  cbind(., convertp(p = .$p.value, digits = 2)) %>% 
  kable(., format = "latex", booktabs = TRUE, 
        digits = c(2, 0, Inf, 2, 2, 2, 2),
        col.names = HLT.col.vnames,
        caption = "Hosmer-Lemeshow Test for Goodness of Fit of Model 2b") %>% 
  footnote(general = FN, general_title = "Note: ", footnote_as_chunk = TRUE,
         threeparttable = TRUE)
```

\FloatBarrier

### Classification 
Table \ref{tab:m2b-classification} presents the classification measures for 
this model. 

```{r m2b-classification}
set.seed(3179) # For reproducible bootstrap estimates.
roc.m2b <- roc(Pass ~ pred.m2b, data = VTAData, ci = TRUE, direction = "<",
               ci.method = "bootstrap", boot.n = 10000)
round(lrcm(roc.m2b, seed = 7146), digits = 3) %>% 
  kable(format = "latex", booktabs = TRUE, 
        caption = "Classification Measures for Model 2b") %>% 
  add_header_above(header = c(" " = 2, "Bootstrapped Quantiles" = 3))
```

\FloatBarrier

### Area Under the Curve (AUC)
Figure \ref{fig:m2b-auc-plot} shows the ROC curve for the model, annotated with 
the best classification threshold for balancing sensitivity versus specificity 
and the area under the curve (AUC). 

```{r m2b-auc-plot, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2b-auc-plot}",
              "Model 2b Receiver Operating Characteristic (ROC) Curve.",
              "The dot marks the best classification threshold.",
              "AUC 95% confidence interval obtained via stratified bootstrap",
              "with 10,000 replicates.")
print(roc.m2b)
plot.roc(roc.m2b, print.auc = TRUE, print.auc.cex = .8, print.thres = "best",
         print.thres.cex = .8)
```

Model 2b shows acceptable ability to discriminate those who pass from those who
fail to pass the level transition testlets, but Table \ref{tab:m2a-m2b-auc-test}
shows that it does not perform better at that than Model 2a.

```{r m2a-m2b-auc-test, message=FALSE}
set.seed(1128) # For reproducible bootstrap estimates.
roct.m2a.m2b <- roc.test(roc.m2a, roc.m2b, method = "bootstrap", boot.n = 10000)
glance(roct.m2a.m2b) %>% 
  cbind(., convertp(p = .$p.value, digits = 2)) %>% 
  select(boot.n, estimate1, estimate2, statistic, p.value, S, BFB, PPH1) %>% 
  kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 3, 3, 2, Inf, 2, 2, 2),
       col.names = c("Bootstrap N", "Model 2a", "Model 2b", "D", "p", "S", 
                     "BFB", "PPH1"),
       caption = paste("Comparing Models 2a and 2b Via Two-Sided, Stratified",
                       "Bootstrap Test for Correlated ROC Curves")) %>% 
  add_header_above(header = c(" ", "AUC Estimates" = 2, " " = 5))
```

\FloatBarrier

### $R^{2}$ Measures
The values for $R^{2}_p = `r pseudoR2(m2b, digits = 2)`$ and 
$R^{2}_{Dev} = `r R2Dev(m2b, digits = 2)`$ are more encouraging than those from 
models 1a and 1b, but identical to those from Model 2a. 

\FloatBarrier

## Diagnostics
We need to run model diagnostics to see if there are any obvious problems with
the model. First we check for outliers and find that there are none of note.

```{r m2b-outlier-test}
outlierTest(m2b)
```

Figure \ref{fig:m2b-resid-plots} shows residual plots for this model. 

```{r m2b-resid-plots, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2b-resid-plots}",
              "Model 2b Residual Plots.")
residualPlots(m2b, layout = c(1,3))
```

Now we look at some index plots for influence measures. We are looking for
observations with values exceeding the cutoffs. Figure \ref{fig:m2b-plotCookD}
shows an index plot of Cook's D for this model.

```{r m2b-plotCookD, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2b-plotCookD}",
              "Model 2b Index Plot of Coook's D.")
PlotCookD(m2b)
```

```{r m2b-high-CookD}
# How many cases have high Cook's Distances (D > cutoff)?
summary(cooks.distance(m2b))
addmargins(table(cooks.distance(m2b) > CookDco(m2b)))
```

Figure \ref{fig:m2b-plot-leverage} shows an index plot of leverage values for 
this model, while Figure \ref{fig:m2b-plot-Pearson-leverage} shows the 
standardized residuals versus the leverage values, with contours for Cook's 
D. 

```{r m2b-plot-leverage, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2b-plot-leverage}",
              "Model 2b Index Plot of Leverage Values.")
PlotHat(m2b)
```

```{r m2b-plot-Pearson-leverage, fig.height=4, fig.width=6, fig.cap=FCap}
FCap <- paste("\\label{fig:m2b-plot-Pearson-leverage}",
              "Model 2b Plot of Standardized Pearson Residuals Versus", 
              "Leverage Values, with Cook's D Contours.")
plot(m2b, which = 5, id.n = 10, cex.id = .6, caption = "", sub.caption = "",
     cook.levels = round(CookDco(m2b), digits = 3))
abline(v = hatco(m2b), lty = 2, col = "blue")
text(x = hatco(m2b), y = 5, pos = 4, col = "blue", cex = .75,
     labels = paste("Leverage cutoff >", round(hatco(m2b), digits = 3)))
```

```{r m2b-high-leverage}
# How many cases have high leverage (hat > cutoff)?
summary(hatvalues(m2b))
addmargins(table(hatvalues(m2b) > hatco(m2b)))
```

```{r m2b-influential-cases}
# Identify cases w/ high leverage and high Cook's D. 
InfCases(m2b)
```

\FloatBarrier

## Graphs

\FloatBarrier

### Interaction Effect of COPIC by Testlet 
We visualize below the transition testlet-specific OPIc effect. Figures
\ref{fig:m2b-plot-Interaction-LogOdds} and \ref{fig:m2b-plot-Interaction-CPR}
respectively plot the centered OPIC score by testlet interaction effect by
showing the simple effect of centered OPIc score separately for each
self-assessment transition testlet on the scale of the linear predictor
(log-odds) and on the response scale (probability or conditional pass rates).
The effect of OPIc decreases as we go from the first to the fourth testlet, as
evidenced by the shallower slope from the left-most to the right-most panel.

```{r m2b-plot-Interaction-LogOdds, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2b-plot-Interaction-LogOdds}",
              "Model 2b Interaction Effect of COPIC by Testlet on Log-Odds", 
              "Scale.")
visreg(m2b, xvar = "COPIC", by = "Testlet", layout = c(4, 1), jitter = TRUE,
       strip.names = TRUE, scale = "linear", 
       xlab = "Centered OPIC Score (OPIC - 5)", 
       ylab = "Log odds (Pass)")
```

```{r m2b-plot-Interaction-CPR, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2b-plot-Interaction-CPR}",
              "Model 2b Interaction Effect of COPIC by Testlet on",
              "Probability Scale.")
visreg(m2b, xvar = "COPIC", by = "Testlet", layout = c(4, 1), jitter = TRUE,
       strip.names = TRUE, scale = "response", rug = 2,
       xlab = "Centered OPIC Score (OPIC - 5)", 
       ylab = "Conditional Pass Rate")
```

\FloatBarrier

### Conditional Pass Rates
Figure \ref{fig:m2b-plot-CPR} plots the conditional pass rates derived from
Model 2b as a function of both testlet and OPIc score.

```{r m2b-plot-CPR, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2b-plot-CPR}",
              "Model 2b Conditional Pass Rates by OPIc Score and",
              "Self-Assessment Testlet.",
              "Lines are labeled with OPIc score.")
m2b.FCPR <- ggplot(m2b.ND[, ShowVars], 
                   aes(Testlet, Pass.Rate, group=OPIC, color=OPIC))+
      geom_line()+ coord_cartesian(ylim = c(0, 1))+
      xlab("Self-Assessment Testlet")+
      ylab("Conditional Pass Rate")
direct.label(m2b.FCPR, "first.points")
```

\FloatBarrier

### Unconditional Pass Rates  
Figure \ref{fig:m2b-plot-UPR} plots the unconditional pass rates derived from
Model 2b as a function of both testlet and OPIc score.

```{r m2b-plot-UPR, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m2b-plot-UPR}",
              "Model 2b Unconditional Pass Rates by OPIc Score and",
              "Self-Assessment Testlet.",
              "Lines are labeled with OPIc score.")
m2b.FUPR <- ggplot(m2b.ND[, ShowVars], 
                   aes(Testlet, Pass.URate, group=OPIC, color=OPIC))+
      geom_line()+ coord_cartesian(ylim = c(0, 1))+
      xlab("Self-Assessment Testlet")+
      ylab("Unconditional Pass Rate")
direct.label(m2b.FUPR, "first.points")
```

\FloatBarrier

## Conclusion
Model 2b is better than Model 2a because the Level x COPIC interaction is
significant. It has better accuracy and sensitivity than Model 1a, but we need
to run another model to decide whether adding a course effect to Model 2b will
improve the model. That will be done in Model 3a.

\FloatBarrier

# Model 3a: Parallel Course + Parallel OPIc Effects 

We now test a model with a parallel course effect and a parallel OPIc
effect. We want to see if course and OPIc are still significant preditors 
of the condtional pass rates when they are simultaneously included into one 
regression model. Model 3a tells us to what extent OPIc and Course each predicts
the conditional pass rates, after controlling for the other predictor's effect. 
Table \ref{tab:m3a-coef} below shows the raw parameter estimates, confidence
intervals, s-values, BFBs, and posterior probabilities of H1 corresponding to
the p-values.

```{r m3a-coef}
m3a %>% 
  tidy(., conf.int = TRUE, conf.level = .95) %>% 
  cbind(., convertp(.$p.value, digits = 2)) %>% 
  kable(format = "latex", booktabs = TRUE, format.args = list(digits = 3), 
        digits = c(2, 2, 2, 2, Inf, 2, 2, 2, 2, 2), 
        col.names = c("Term", "Estimate", "SE", "z-value", "p-value", "CI.LL", 
                      "CI.UL", "S", "BFB", "PPH1"),
        caption = "Model 3a Coefficients")
```

\FloatBarrier

## Sequential Tests (Type I SS)
Each row in Table \ref{tab:m3a-Type-I} tests the significance of unique
additional variance explained by the term on that line after controlling for all
previously entered terms. Significant results mean adding that term improved the
model.

```{r m3a-Type-I}
m3a %>%
 anova(., test = "Chisq") %>%
 cbind(., convertp(.[,"Pr(>Chi)"])) %>%
 kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 2, 0, 2, Inf, 2, 2, 2),
       col.names = c("DF", "Deviance", "Resid. DF", "Resid. Dev", "p-value",
                     "S", "BFB", "PPH1"),
       caption = "Model 3a Sequential Tests (Type I SS): Analysis of Deviance")
```

Note that Course entered the model last, and it was not significant. This 
indicated that Course did not explain a significant amount of the variance 
in the conditional pass rates after controlling for COPIC.

\FloatBarrier

## Simultaneous Tests of Main/Interaction Effects via LRT (Type III SS)
The simultaneous tests in Table \ref{tab:m3a-Type-III} are the effects of the
indicated terms after controlling for all other terms in the model. They are
only computed for terms that are not part of a higher-order interaction because
it makes no sense to test for a main effect when the variable is involved in an
interaction. These should be functionally equivalent to the results you get with
*anova()* when you feed it a pair of nested models that differ only in that one
model includes a term that is absent from the other model. They are likelihood
ratio tests (LRTs).

```{r m3a-Type-III}
m3a %>% 
  drop1(., test = "Chisq") %>%
  cbind(., convertp(.[,"Pr(>Chi)"])) %>% 
  kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 2, 2, 2, Inf, 2, 2, 2),
       col.names = c("DF", "Deviance", "AIC", "LRT", "p-value", "S", "BFB",
                     "PPH1"),
       caption = "Model 3a Simultaneous Tests (Type III SS)")
```

Results of the simultaneous tests are consistent with those of the sequential 
tests in showing that Course was no longer a significant predictor of the 
conditional pass rates after controlling for COPIC.

The fact that the course effect is not significant in Table 
\ref{tab:m3a-Type-III} suggest that we can revert back to Model 2a because
Course doesn't have an effect after controlling for level and COPIC.

\FloatBarrier

## Conditional and Unconditional Pass Rates
We omit calculating these rates because Model 3a is not better than Model 2a.

\FloatBarrier

## Odds-Ratio for COPIC Effect 
We omit calculating odds-ratios because Model 3a is not better than Model 2a.

\FloatBarrier

## Assessing Goodness of Fit, Discrimination, and Calibration
Since we are running the continuation-ration model as a logistic regression, all
the stadard methods for assessing goodness of fit for logistic regresion models
apply.

\FloatBarrier

### Hosmer-Lemeshow Goodness of Fit Test
Figure \ref{fig:m3a-calib-plot} shows a calibration plot for this model. The 
plot is based on the bins summarized in Table \ref{tab:m3a-bins}, while the
Hosmer-Lemeshow test is shown in Table \ref{tab:m3a-HLT}. 

```{r m3a-calib-plot, message=FALSE, fig.height=4, fig.width=6, fig.cap=FCap}
FCap <- paste("\\label{fig:m3a-calib-plot}",
              "Model 3a Calibration Plot with Bin Sample Sizes and",
              "Hosmer-Lemeshow Test.")
HLT.m3a <- HLfit(m3a, bin.method = "prob.bins", min.prob.interval = 0.1, 
                 xlab = "Predicted Probability",
                 ylab = "Observed Probability")
```

```{r m3a-bins}
FN <- "Minimum bin interval width = 0.10."

HLT.m3a$bins.table %>% 
  kable(., format = "latex", booktabs = TRUE, digits = 3, row.names = FALSE,
        col.names = HLT.bin.vnames,
        caption = "Calibration Bins Used for Model 3a Hosmer-Lemeshow Test") %>% 
  add_header_above(., header = c(" " = 4, "Obs. Prop. 95% CI" = 2)) %>% 
  column_spec(column = 1, width = "1.75cm") %>% 
  column_spec(column = 3:4, width = "1.75cm") %>% 
  column_spec(column = 5:6, width = "1.25cm") %>% 
  footnote(general = FN, general_title = "Note: ", footnote_as_chunk = TRUE,
         threeparttable = TRUE)
```

```{r m3a-HLT}
FN <- "Minimum bin interval width = 0.10."

HLT.m3a %>% 
  as_tibble() %>%  
  select(chi.sq, DF, p.value, RMSE) %>% 
  unique() %>% 
  cbind(., convertp(p = .$p.value, digits = 2)) %>% 
  kable(., format = "latex", booktabs = TRUE, 
        digits = c(2, 0, Inf, 2, 2, 2, 2),
        col.names = HLT.col.vnames,
        caption = "Hosmer-Lemeshow Test for Goodness of Fit of Model 3a") %>% 
  footnote(general = FN, general_title = "Note: ", footnote_as_chunk = TRUE,
         threeparttable = TRUE)
```

\FloatBarrier

### Classification 
Table \ref{tab:m3a-classification} presents the classification measures for 
this model. 


```{r m3a-classification}
set.seed(3419) # For reproducibility of bootstrap estimates.
roc.m3a <- roc(Pass ~ pred.m3a, data = VTAData, ci = TRUE, direction = "<",
               ci.method = "bootstrap", boot.n = 10000)
round(lrcm(roc.m3a, seed = 6342), digits = 3) %>% 
  kable(format = "latex", booktabs = TRUE, 
        caption = "Classification Measures for Model 3a") %>% 
  add_header_above(header = c(" " = 2, "Bootstrapped Quantiles" = 3))
```

\FloatBarrier

### Area Under the Curve (AUC)
Figure \ref{fig:m3a-auc-plot} shows the ROC curve for the model, annotated with 
the best classification threshold for balancing sensitivity versus specificity 
and the area under the curve (AUC). 

```{r m3a-auc-plot, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m3a-auc-plot}",
              "Model 3a Receiver Operating Characteristic (ROC) Curve.",
              "The dot marks the best classification threshold.",
              "AUC 95% confidence interval obtained via stratified bootstrap",
              "with 10,000 replicates.")
print(roc.m3a)
plot.roc(roc.m3a, print.auc = TRUE, print.auc.cex = .8, print.thres = "best",
         print.thres.cex = .8)
```

Model 3a shows acceptable ability to discriminate those who pass from those who
fail to pass the level transition testlets, but Table \ref{tab:m2a-m3a-auc-test}
shows that it does not perform better at that than Model 2a.

```{r m2a-m3a-auc-test, message=FALSE}
set.seed(1346) # For reproducible bootstrap estimates.
roct.m2a.m3a <- roc.test(roc.m2a, roc.m3a, method = "bootstrap", boot.n = 10000)
glance(roct.m2a.m3a) %>% 
  cbind(., convertp(p = .$p.value, digits = 2)) %>% 
  select(boot.n, estimate1, estimate2, statistic, p.value, S, BFB, PPH1) %>% 
  kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 3, 3, 2, Inf, 2, 2, 2),
       col.names = c("Bootstrap N", "Model 2a", "Model 3a", "D", "p", "S", 
                     "BFB", "PPH1"),
       caption = paste("Comparing Models 2a and 3a Via Two-Sided, Stratified",
                       "Bootstrap Test for Correlated ROC Curves")) %>% 
  add_header_above(header = c(" ", "AUC Estimates" = 2, " " = 5))
```

\FloatBarrier

### $R^{2}$ Measures
The values for $R^{2}_p = `r pseudoR2(m3a, digits = 2)`$ and 
$R^{2}_{Dev} = `r R2Dev(m3a, digits = 2)`$ are more encouraging than those from 
models 1a and 1b, but identical to those from models 2a and 2b. 

\FloatBarrier

## Diagnostics
We omit diagnostics because Model 3a is not better than Model 2a.

\FloatBarrier

## Graphs
We omit graphs because Model 3a is not better than Model 2a. 

\FloatBarrier

## Conclusion
What distinguished this model from Model 2a was inclusion of the parallel course
effect, but the Type III LRT for that effect is not-significant. We will report
Model 2a over this model on the basis of parsimony.


# Model 3b: Parallel Course + Non-parallel OPIc Effects
We finally test a model with a parallel course effect and a non-parallel OPIc
effect. We add the course effect into the model after the interaction so that a
sequential test will examine whether it adds any value beyond the non-parallel
OPIC effect. Table \ref{tab:m3b-coef} below shows the raw parameter estimates,
confidence intervals, s-values, BFBs, and posterior probabilities of H1 
corresponding to the p-values.

```{r m3b-coef}
# T38 <- summary(m3b)
# set.caption("Model 3a Summary")
# pander(T38, digits = c(5, 5, 4, 3), round = c(4, 4, 2, Inf), justify = "right",
#        caption = "Model 3a Summary")

m3b %>% 
  tidy(., conf.int = TRUE, conf.level = .95) %>% 
  cbind(., convertp(.$p.value, digits = 2)) %>% 
  kable(format = "latex", booktabs = TRUE, format.args = list(digits = 3), 
        digits = c(2, 2, 2, 2, Inf, 2, 2, 2, 2, 2), 
        col.names = c("Term", "Estimate", "SE", "z-value", "p-value", "CI.LL", 
                      "CI.UL", "S", "BFB", "PPH1"),
        caption = "Model 3b Coefficients")
```

\FloatBarrier

## Sequential Tests (Type I SS)
Each row in Table \ref{tab:m3b-Type-I} tests the significance of unique
additional variance explained by the term on that line after controlling for all
previously entered terms. Significant results mean adding that term improved the
model. Note that course actually entered the model last.

```{r m3b-Type-I}
m3b %>%
 anova(., test = "Chisq") %>%
 cbind(., convertp(.[,"Pr(>Chi)"])) %>%
 kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 2, 0, 2, Inf, 2, 2, 2),
       col.names = c("DF", "Deviance", "Resid. DF", "Resid. Dev", "p-value",
                     "S", "BFB", "PPH1"),
       caption = "Model 3b Sequential Tests (Type I SS): Analysis of Deviance")
```

\FloatBarrier

## Simultaneous Tests of Main/Interaction Effects via LRT (Type III SS)
The simultaneous tests in Table \ref{tab:m3b-Type-III} are the effects of the
indicated terms after controlling for all other terms in the model. They are
only computed for terms that are not part of a higher-order interaction because
it makes no sense to test for a main effect when the variable is involved in an
interaction. These should be functionally equivalent to the results you get with
*anova()* when you feed it a pair of nested models that differ only in that one
model includes a term that is absent from the other model. They are likelihood
ratio tests (LRTs).

```{r m3b-Type-III}
m3b %>% 
  drop1(., test = "Chisq") %>%
  cbind(., convertp(.[,"Pr(>Chi)"])) %>% 
  kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 2, 2, 2, Inf, 2, 2, 2),
       col.names = c("DF", "Deviance", "AIC", "LRT", "p-value", "S", "BFB",
                     "PPH1"),
       caption = "Model 3b Simultaneous Tests (Type III SS)")
```

```{r m3a-m3b-LRT}
anova(m3a, m3b, test = "Chisq") %>%  
  cbind(., convertp(.[,"Pr(>Chi)"])) %>% 
  kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 2, 0, 2, Inf, 2, 2, 2),
       col.names = c("Resid. Df", "Resid. Deviance", "Df", "Deviance", 
                     "p-value", "S", "BFB", "PPH1"),
       caption = "Model 3a vs 3b: Likelihood Ratio Test for Interaction")
```

The fact that the course effect is not significant in Table
\ref{tab:m3b-Type-III} suggests that we can revert back to Model 2a because
course doesn't have an effect after controlling for testlet, COPIC, and their
interaction.

\FloatBarrier

## Conditional and Unconditional Pass Rates
We omit calculating these rates because Model 3b is not better than Model 2b.

\FloatBarrier

## Odds-Ratio for COPIC Effect 
We omit calculating odds-ratios because Model 3b is not better than Model 2b.

\FloatBarrier

## Assessing Goodness of Fit, Discrimination, and Calibration

\FloatBarrier

### Hosmer-Lemeshow Goodness of Fit Test 
Figure \ref{fig:m3b-calib-plot} shows a calibration plot for this model. The 
plot is based on the bins summarized in Table \ref{tab:m3b-bins}, while the
Hosmer-Lemeshow test is shown in Table \ref{tab:m3b-HLT}. 

```{r m3b-calib-plot, message=FALSE, fig.height=4, fig.width=6, fig.cap=FCap}
FCap <- paste("\\label{fig:m3b-calib-plot}",
              "Model 3b Calibration Plot with Bin Sample Sizes and",
              "Hosmer-Lemeshow Test.")
HLT.m3b <- HLfit(m3b, bin.method = "prob.bins", min.prob.interval = 0.1, 
                 xlab = "Predicted Probability",
                 ylab = "Observed Probability")
```

```{r m3b-bins}
FN <- "Minimum bin interval width = 0.10."

HLT.m3b$bins.table %>% 
  kable(., format = "latex", booktabs = TRUE, digits = 3, row.names = FALSE,
        col.names = HLT.bin.vnames,
        caption = "Calibration Bins Used for Model 3b Hosmer-Lemeshow Test") %>% 
  add_header_above(., header = c(" " = 4, "Obs. Prop. 95% CI" = 2)) %>% 
  column_spec(column = 1, width = "1.75cm") %>% 
  column_spec(column = 3:4, width = "1.75cm") %>% 
  column_spec(column = 5:6, width = "1.25cm") %>% 
  footnote(general = FN, general_title = "Note: ", footnote_as_chunk = TRUE,
         threeparttable = TRUE)
```

```{r m3b-HLT}
FN <- "Minimum bin interval width = 0.10."

HLT.m3b %>% 
  as_tibble() %>%  
  select(chi.sq, DF, p.value, RMSE) %>% 
  unique() %>% 
  cbind(., convertp(p = .$p.value, digits = 2)) %>% 
  kable(., format = "latex", booktabs = TRUE, 
        digits = c(2, 0, Inf, 2, 2, 2, 2),
        col.names = HLT.col.vnames,
        caption = "Hosmer-Lemeshow Test for Goodness of Fit of Model 3b") %>% 
  footnote(general = FN, general_title = "Note: ", footnote_as_chunk = TRUE,
         threeparttable = TRUE)
```

\FloatBarrier

### Classification 
Table \ref{tab:m3b-classification} presents the classification measures for 
this model. 

```{r m3b-classification}
set.seed(9173) # For reproducibility of bootstrap estimates.
roc.m3b <- roc(Pass ~ pred.m3b, data = VTAData, ci = TRUE, direction = "<",
               ci.method = "bootstrap", boot.n = 10000)
round(lrcm(roc.m3b, seed = 7146), digits = 3) %>% 
  kable(format = "latex", booktabs = TRUE, 
        caption = "Classification Measures for Model 3b") %>% 
  add_header_above(header = c(" " = 2, "Bootstrapped Quantiles" = 3))
```

\FloatBarrier

### Area Under the Curve (AUC)
Figure \ref{fig:m3b-auc-plot} shows the ROC curve for the model, annotated with 
the best classification threshold for balancing sensitivity versus specificity 
and the area under the curve (AUC). 

```{r m3b-auc-plot, fig.height=3, fig.width=5, fig.cap=FCap}
FCap <- paste("\\label{fig:m3b-auc-plot}",
              "Model 3b Receiver Operating Characteristic (ROC) Curve.",
              "The dot marks the best classification threshold.",
              "AUC 95% confidence interval obtained via stratified bootstrap",
              "with 10,000 replicates.")
print(roc.m3b)
plot.roc(roc.m3b, print.auc = TRUE, print.auc.cex = .8, print.thres = "best",
         print.thres.cex = .8)
```

Model 3b shows acceptable ability to discriminate those who pass from those who
fail to pass the level transition testlets, but Table \ref{tab:m2a-m3b-auc-test}
shows that it does not perform better at that than Model 2a.

```{r m2a-m3b-auc-test, message=FALSE}
set.seed(1128) # For reproducible bootstrap estimates.
roct.m2a.m3b <- roc.test(roc.m2a, roc.m3b, method = "bootstrap", boot.n = 10000)
glance(roct.m2a.m3b) %>% 
  cbind(., convertp(p = .$p.value, digits = 2)) %>% 
  select(boot.n, estimate1, estimate2, statistic, p.value, S, BFB, PPH1) %>% 
  kable(format = "latex", booktabs = TRUE, 
       digits = c(0, 3, 3, 2, Inf, 2, 2, 2),
       col.names = c("Bootstrap N", "Model 2a", "Model 3b", "D", "p", "S", 
                     "BFB", "PPH1"),
       caption = paste("Comparing Models 2a and 3b Via Two-Sided, Stratified",
                       "Bootstrap Test for Correlated ROC Curves")) %>% 
  add_header_above(header = c(" ", "AUC Estimates" = 2, " " = 5))
```

\FloatBarrier

### $R^{2}$ Measures
The values for $R^{2}_p = `r pseudoR2(m3b, digits = 2)`$ and 
$R^{2}_{Dev} = `r R2Dev(m3b, digits = 2)`$ are more encouraging than those from 
models 1a and 1b, but identical to those from models 2a, 2b, and 3a.

\FloatBarrier

## Diagnostics
We omit diagnostics because Model 3b is not better than Model 2b.

\FloatBarrier

## Graphs
We omit graphs because Model 3b is not better than Models 2a and 2b. 

\FloatBarrier

## Conclusion
What distinguished this model from Model 2b was inclusion of the parallel course
effect, but the Type III LRT for that effect is not-significant. We will report
Model 2b over this model on the basis of parsimony.

\FloatBarrier

# References
Benjamin, D. J., & Berger, J. O. (2019). Three recommendations for improving 
the use of p-values. *The American Statistician, 73*(Supplement 1), 186-191. 
https://doi.org/10.1080/00031305.2018.1543135

Cameron, A. C., & Windmeijer, F. A. G. (1997). An R-squared measure of goodness
of fit for some common nonlinear regression models. 
*Journal of Econometrics, 77*(2), 329-342. 
https://doi.org/10.1016/S0304-4076(96)01818-0

Colquhoun, D. (2019). The false positive risk: A proposal concerning what to 
do about p-values. *The American Statistician, 73*(Supplement 1), 192-201. 
https://doi-org/10.1080/00031305.2018.1529622

Fenlon, C., OGrady, L., Doherty, M. L., & Dunnion, J. (2018). A discussion of 
calibration techniques for evaluating binary and categorical predictive models.
*Preventive Veterinary Medicine, 149*, 107-114. 
https://doi.org/10.1016/j.prevetmed.2017.11.018

Fox, J. (1997). *Applied regression analysis, linear models, and related methods*. 
Thousand Oaks, CA: Sage Publications.

Greenland, S. (2019). Valid p-values behave exactly as they should: Some 
misleading criticisms of p-values and their resolution with s-values. 
*The American Statistician, 73*(Supplement 1), 106-114. 
https://doi.org/10.1080/00031305.2018.1529625

Hosmer, D. W., Lemeshow, S., & Sturdivant, R. X. (2013). 
*Applied logistic regression* (3rd ed.). Hoboken, NJ: John Wiley & Sons, Inc.

Perkins, N. J., & Schisterman, E. F. (2006). The inconsistency of optimal
cutpoints obtained using two criteria based on the receiver operating
characteristic curve. *American Journal of Epidemiology, 163*(7), 670-675.
https://doi-org/10.1093/aje/kwj063

Raykov, T., & Marcoulides, G. A. (2011). *Introduction to psychometric theory*.
New York, NY: Routledge.

Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., & 
Mller, M. (2011). pROC: an open-source package for R and S+ to analyze and 
compare ROC curves. *BMC Bioinformatics, 12*, 77. 
https://doi.org/10.1186/1471-2105-12-77

Steyerberg, E. W., Harrell Jr., F. E., Borsboom, G. J. J. M., Eijkemans, M. J. 
C., Vergouwe, Y., & Habbema, J. D. F. (2001). Internal validation of predictive 
models: Efficiency of some procedures for logistic regression analysis. 
*Journal of Clinical Epidemiology, 54*(8), 774-781. 
https://doi.org/10.1016/S0895-4356(01)00341-9

Steyerberg, E. W., Vickers, A. J., Cook, N. R., Gerds, T., Gonen, M., 
Obuchowski, N. A., Pencina, M. J., Kattan, M. W. (2010). Assessing the 
performance of prediction models: A framework for traditional and novel 
measures. *Epidemiology, 21*(1), 128-138. 
http://doi.org/10.1097/EDE.0b013e3181c30fb2

Wasserstein, R. L., Schirm, A. L., & Lazar, N. A. (2019). Moving to a world 
beyond "p < .05". *The American Statistician, 73*(Supplement 1), 1-19. 
https://doi.org/10.1080/00031305.2019.1583913

Westfall, P. H. (1997). Multiple testing of general contrasts using logical 
constraints and correlations. 
*Journal of the American Statistical Association, 92*(437), 299-306. https://doi.org/10.1080/01621459.1997.10473627

Winke, P., & Zhang, X. (2022, March 14). 
*Data and codebook for SSLA article: A closer look at a marginalized test method: Self-assessment as a measure of speaking proficiency.* 
(Study 164981; Version V1) [Data files and codebooks]. Inter-university
Consortium for Political and Social Research. https://doi.org/10.3886/E164981V1

Winke, P., Zhang, X., & Pierce, S. J. (2022). A closer look at a marginalized 
test method: Self-assessment as a measure of speaking proficiency [Manuscript 
accepted for publication]. *Studies in Second Language Acquisition*.

Winke, P., Pierce, S. J., & Zhang, X. (2018, October). Self-assessment works! 
Continuation-ratio models for testing course and OPIc score effects on oral 
proficiency self-assessments. Paper presented at the East Coast Organization 
of Language Testers 2018 conference, hosted by the Educational Testing Service,
Princeton, NJ. https://sites.google.com/site/ecoltaelrc/home

Youden, W. J. (1950). Index for rating diagnostic tests. *Cancer, 3*(1), 32-35.
https://doi.org/10.1002/1097-0142(1950)3%3A1%3C32%3A%3AAID-CNCR2820030106%3E3.0.CO%3B2-3

\FloatBarrier

# Software Information
We use R Markdown to enhance reproducibility. Knitting the source R Markdown 
script generates the PDF file containing explanatory text, R code, and R output.

- We used [RStudio](www.rstudio.org) `r rstudioapi::versionInfo()$version` to 
  work with R and R markdown files. 
- Our software chain looks like this:
  **Rmd file > RStudio > R > rmarkdown > knitr > md file > pandoc > tex file > TinyTeX > PDF file**.
- Source script: *`r knitr:::current_input()`*
- Output file: *`r sub(".Rmd", ".pdf", knitr:::current_input(dir = FALSE))`*
- We recommend using [TinyTeX](https://yihui.org/tinytex/) to compile LaTeX 
  files into PDF files. However, it should be viable to use 
  [MiKTeX](https://miktex.org) instead. 
- We used [pandoc](https://pandoc.org) `r pandoc_version()` for this 
  document. 

This document was generated using the following computational environment and 
dependencies: 

``` {r show-citations}
# Check and report whether we used TinyTex or other LaTeX software. 
which_latex()

# Get R and R package version numbers in use.
devtools::session_info()
```

The current Git commit details and status are:

```{r git-details, echo = TRUE, eval=TRUE}
git_report()
```
